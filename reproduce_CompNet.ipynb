{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDa1e4H6Tfy3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('CompositionalNets')\n",
    "COMPOSITIONAL_NETS_PATH = './'\n",
    "CODE_PATH = os.path.join(COMPOSITIONAL_NETS_PATH, 'Code')\n",
    "DATA_PATH = os.path.join(COMPOSITIONAL_NETS_PATH, 'data')\n",
    "MODELS_PATH = os.path.join(COMPOSITIONAL_NETS_PATH, 'models')\n",
    "import sys\n",
    "sys.path.append(CODE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pdzrov_Gdl0w"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from config import categories, categories_train, dataset, data_path, device_ids, mix_model_path, dict_dir, layer, vMF_kappa, model_save_dir, compnet_type, backbone_type, num_mixtures\n",
    "from config import config as cfg\n",
    "from model import Net\n",
    "from helpers import getVmfKernels, getCompositionModel, update_clutter_model, myresize\n",
    "from eval_occlusion_localization import visualize_response_map\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "import pdb\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import pdb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from model import Net\n",
    "from helpers import save_checkpoint,getCompositionModel,getVmfKernels, update_clutter_model\n",
    "from config import device_ids, mix_model_path, categories, categories_train, dict_dir, dataset, data_path, layer, vc_num, model_save_dir, compnet_type,backbone_type, vMF_kappa,num_mixtures\n",
    "from config import config as cfg\n",
    "from torch.utils.data import DataLoader\n",
    "from losses import ClusterLoss\n",
    "from model import resnet_feature_extractor\n",
    "import torchvision.models as models\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImg(archive, mode, categories, dataset, data_path, \n",
    "           cat_test=None, occ_level='ZERO', occ_type=None, bool_load_occ_mask = False):\n",
    "\n",
    "    if mode == 'train':\n",
    "        train_imgs = []\n",
    "        train_labels = []\n",
    "        train_masks = []\n",
    "        for category in categories:\n",
    "            if dataset == 'pascal3d+':\n",
    "                if occ_level == 'ZERO':\n",
    "                    filelist = 'pascal3d+_occ/' + category + '_imagenet_train' + '.txt'\n",
    "                    img_dir = 'pascal3d+_occ/TRAINING_DATA/' + category + '_imagenet'\n",
    "            elif dataset == 'coco':\n",
    "                if occ_level == 'ZERO':\n",
    "                    img_dir = 'coco_occ/{}_zero'.format(category)\n",
    "                    filelist = 'coco_occ/{}_{}_train.txt'.format(category, occ_level)\n",
    "\n",
    "            with archive.open(filelist, 'r') as fh:\n",
    "                contents = fh.readlines()\n",
    "            img_list = [cc.strip().decode('ascii') for cc in contents]\n",
    "            label = categories.index(category)\n",
    "            for img_path in img_list:\n",
    "                if dataset=='coco':\n",
    "                    if occ_level == 'ZERO':\n",
    "                        img = img_dir + '/' + img_path + '.jpg'\n",
    "                    else:\n",
    "                        img = img_dir + '/' + img_path + '.JPEG'\n",
    "                else:\n",
    "                    img = img_dir + '/' + img_path + '.JPEG'\n",
    "                occ_img1 = []\n",
    "                occ_img2 = []\n",
    "                train_imgs.append(img)\n",
    "                train_labels.append(label)\n",
    "                train_masks.append([occ_img1,occ_img2])\n",
    "        \n",
    "        return train_imgs, train_labels, train_masks\n",
    "\n",
    "    else:\n",
    "        test_imgs = []\n",
    "        test_labels = []\n",
    "        occ_imgs = []\n",
    "        for category in cat_test:\n",
    "            if dataset == 'pascal3d+':\n",
    "                filelist = data_path + 'pascal3d+_occ/' + category + '_imagenet_occ.txt'\n",
    "                img_dir = data_path + 'pascal3d+_occ/' + category + 'LEVEL' + occ_level\n",
    "                if bool_load_occ_mask:\n",
    "                    if  occ_type=='':\n",
    "                        occ_mask_dir = 'pascal3d+_occ/' + category + 'LEVEL' + occ_level+'_mask_object'\n",
    "                    else:\n",
    "                        occ_mask_dir = 'pascal3d+_occ/' + category + 'LEVEL' + occ_level+'_mask'\n",
    "                    occ_mask_dir_obj = 'pascal3d+_occ/0_old_masks/'+category+'_imagenet_occludee_mask/'\n",
    "            elif dataset == 'coco':\n",
    "                if occ_level == 'ZERO':\n",
    "                    img_dir = 'coco_occ/{}_zero'.format(category)\n",
    "                    filelist = 'coco_occ/{}_{}_test.txt'.format(category, occ_level)\n",
    "                else:\n",
    "                    img_dir = 'coco_occ/{}_occ'.format(category)\n",
    "                    filelist = 'coco_occ/{}_{}.txt'.format(category, occ_level)\n",
    "\n",
    "#             if os.path.exists(filelist):\n",
    "            with archive.open(filelist, 'r') as fh:\n",
    "                contents = fh.readlines()\n",
    "            img_list = [cc.strip().decode('ascii') for cc in contents]\n",
    "            label = categories.index(category)\n",
    "            for img_path in img_list:\n",
    "                if dataset != 'coco':\n",
    "                    if occ_level=='ZERO':\n",
    "                        img = img_dir + occ_type + '/' + img_path[:-2] + '.JPEG'\n",
    "                        occ_img1 = []\n",
    "                        occ_img2 = []\n",
    "                    else:\n",
    "                        img = img_dir + occ_type + '/' + img_path + '.JPEG'\n",
    "                        if bool_load_occ_mask:\n",
    "                            occ_img1 = occ_mask_dir + '/' + img_path + '.JPEG'\n",
    "                            occ_img2 = occ_mask_dir_obj + '/' + img_path + '.png'\n",
    "                        else:\n",
    "                            occ_img1 = []\n",
    "                            occ_img2 = []\n",
    "\n",
    "                else:\n",
    "                    img = img_dir + occ_type + '/' + img_path + '.jpg'\n",
    "                    occ_img1 = []\n",
    "                    occ_img2 = []\n",
    "\n",
    "                test_imgs.append(img)\n",
    "                test_labels.append(label)\n",
    "                occ_imgs.append([occ_img1,occ_img2])\n",
    "#             else:\n",
    "#                 print('FILELIST NOT FOUND: {}'.format(filelist))\n",
    "        return test_imgs, test_labels, occ_imgs\n",
    "\n",
    "\n",
    "def train(model, train_data, val_data, epochs, batch_size, learning_rate, savedir, occ_types, alpha=3,beta=3, vc_flag=True, mix_flag=False):\n",
    "    best_check = {\n",
    "        'epoch': 0,\n",
    "        'best': 0,\n",
    "        'val_acc': 0\n",
    "    }\n",
    "    out_file_name = savedir + 'result.txt'\n",
    "    total_train = len(train_data)\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=1, shuffle=True)\n",
    "    val_loaders=[]\n",
    "\n",
    "    for i in range(len(val_data)):\n",
    "        val_loader = DataLoader(dataset=val_data[i], batch_size=1, shuffle=True)\n",
    "        val_loaders.append(val_loader)\n",
    "\n",
    "    # we observed that training the backbone does not make a very big difference but not training saves a lot of memory\n",
    "    # if the backbone should be trained, then only with very small learning rate e.g. 1e-7\n",
    "    for param in model.backbone.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    if not vc_flag:\n",
    "        model.conv1o1.weight.requires_grad = False\n",
    "    else:\n",
    "        model.conv1o1.weight.requires_grad = True\n",
    "\n",
    "    if not mix_flag:\n",
    "        model.mix_model.requires_grad = False\n",
    "    else:\n",
    "        model.mix_model.requires_grad = True\n",
    "\n",
    "    classification_loss = nn.CrossEntropyLoss()\n",
    "    cluster_loss = ClusterLoss()\n",
    "\n",
    "    optimizer = torch.optim.Adagrad(params=filter(lambda param: param.requires_grad, model.parameters()), lr=learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer,gamma=0.98)\n",
    "\n",
    "    print('Training')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        out_file = open(out_file_name, 'a')\n",
    "        train_loss = 0.0\n",
    "        correct = 0\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        model.backbone.eval()\n",
    "        for index, data in enumerate(train_loader):\n",
    "            if index % 500 == 0 and index != 0:\n",
    "                end = time.time()\n",
    "                print('Epoch{}: {}/{}, Acc: {}, Loss: {} Time:{}'.format(epoch + 1, index, total_train, correct.cpu().item() / index, train_loss.cpu().item() / index, (end-start)))\n",
    "                start = time.time()\n",
    "\n",
    "            input, _, label = data\n",
    "\n",
    "            input = input.cuda(device_ids[0])\n",
    "            label = label.cuda(device_ids[0])\n",
    "\n",
    "            output, vgg_feat, like = model(input)\n",
    "\n",
    "            out = output.argmax(1)\n",
    "            correct += torch.sum(out == label)\n",
    "            class_loss = classification_loss(output, label) / output.shape[0]\n",
    "\n",
    "            loss = class_loss\n",
    "            if alpha != 0:\n",
    "                clust_loss = cluster_loss(vgg_feat, model.conv1o1.weight) / output.shape[0]\n",
    "                loss += alpha * clust_loss\n",
    "\n",
    "            if beta!=0:\n",
    "                mix_loss = like[0,label[0]]\n",
    "                loss += -beta *mix_loss\n",
    "\n",
    "            #with torch.autograd.set_detect_anomaly(True):\n",
    "            loss.backward()\n",
    "\n",
    "            # pseudo batches\n",
    "            if np.mod(index,batch_size)==0:# and index!=0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            train_loss += loss.detach() * input.shape[0]\n",
    "\n",
    "        updated_clutter = update_clutter_model(model,device_ids, occ_types)\n",
    "        model.clutter_model = updated_clutter\n",
    "        scheduler.step()\n",
    "        train_acc = correct.cpu().item() / total_train\n",
    "        train_loss = train_loss.cpu().item() / total_train\n",
    "        out_str = 'Epochs: [{}/{}], Train Acc:{}, Train Loss:{}'.format(epoch + 1, epochs, train_acc, train_loss)\n",
    "        print(out_str)\n",
    "        out_file.write(out_str)\n",
    "\n",
    "        # Evaluate Validation images\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            correct = 0\n",
    "            val_accs=[]\n",
    "            for i in range(len(val_loaders)):\n",
    "                val_loader = val_loaders[i]\n",
    "                correct_local=0\n",
    "                total_local = 0\n",
    "                val_loss = 0\n",
    "                out_pred = torch.zeros(len(val_data[i].images))\n",
    "                for index, data in enumerate(val_loader):\n",
    "                    input,_, label = data\n",
    "                    input = input.cuda(device_ids[0])\n",
    "                    label = label.cuda(device_ids[0])\n",
    "                    output,_,_ = model(input)\n",
    "                    out = output.argmax(1)\n",
    "                    out_pred[index] = out\n",
    "                    correct_local += torch.sum(out == label)\n",
    "                    total_local += label.shape[0]\n",
    "\n",
    "                    class_loss = classification_loss(output, label) / output.shape[0]\n",
    "                    loss = class_loss\n",
    "                    val_loss += loss.detach() * input.shape[0]\n",
    "                correct += correct_local\n",
    "                val_acc = correct_local.cpu().item() / total_local\n",
    "                val_loss = val_loss.cpu().item() / total_local\n",
    "                val_accs.append(val_acc)\n",
    "                out_str = 'Epochs: [{}/{}], Val-Set {}, Val Acc:{} Val Loss:{}\\n'.format(epoch + 1, epochs,i , val_acc,val_loss)\n",
    "                print(out_str)\n",
    "                out_file.write(out_str)\n",
    "            val_acc = np.mean(val_accs)\n",
    "            out_file.write('Epochs: [{}/{}], Val Acc:{}\\n'.format(epoch + 1, epochs, val_acc))\n",
    "            if val_acc>best_check['val_acc']:\n",
    "                print('BEST: {}'.format(val_acc))\n",
    "                out_file.write('BEST: {}\\n'.format(val_acc))\n",
    "                best_check = {\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'val_acc': val_acc,\n",
    "                    'epoch': epoch\n",
    "                }\n",
    "                save_checkpoint(best_check, savedir + 'vc' + str(epoch + 1) + '.pth', True)\n",
    "\n",
    "            print('\\n')\n",
    "        out_file.close()\n",
    "    return best_check\n",
    "\n",
    "\n",
    "def imgLoader(archive, img_path,mask_path,bool_resize_images=True,bool_square_images=False):\n",
    "    \n",
    "    archive_img_path = archive.open(img_path)\n",
    "    input_image = Image.open(archive_img_path)\n",
    "    if bool_resize_images:\n",
    "        if bool_square_images:\n",
    "            input_image.resize((224,224),Image.ANTIALIAS)\n",
    "        else:\n",
    "            sz=input_image.size\n",
    "            min_size = np.min(sz)\n",
    "            if min_size!=224:\n",
    "                input_image = input_image.resize((np.asarray(sz) * (224 / min_size)).astype(int),Image.ANTIALIAS)\n",
    "    preprocess =  transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    img = preprocess(input_image)\n",
    "\n",
    "    if mask_path[0]:\n",
    "        f = archive.open(mask_path[0])\n",
    "        mask1 = np.array(Image.open(f))\n",
    "        f.close()\n",
    "        mask1 = myresize(mask1, 224, 'short')\n",
    "        try:\n",
    "            mask2 = cv2.imread(mask_path[1])[:, :, 0]\n",
    "            mask2 = mask2[:mask1.shape[0], :mask1.shape[1]]\n",
    "        except:\n",
    "            mask = mask1\n",
    "        try:\n",
    "            mask = ((mask1 == 255) * (mask2 == 255)).astype(np.float)\n",
    "        except:\n",
    "            mask = mask1\n",
    "    else:\n",
    "        mask = np.ones((img.shape[0], img.shape[1])) * 255.0\n",
    "\n",
    "    mask = torch.from_numpy(mask)\n",
    "    return img,mask\n",
    "\n",
    "\n",
    "class Imgset():\n",
    "    def __init__(self, archive, imgs, masks, labels, loader,bool_square_images=False):\n",
    "        self.archive = archive\n",
    "        self.images = imgs\n",
    "        self.masks \t= masks\n",
    "        self.labels = labels\n",
    "        self.loader = loader\n",
    "        self.bool_square_images = bool_square_images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        mask = self.masks[index]\n",
    "        img,mask = self.loader(self.archive,fn,mask,bool_resize_images=True,bool_square_images=self.bool_square_images)\n",
    "        return img, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "def save_checkpoint(state, filename, is_best):\n",
    "    if is_best:\n",
    "        print(\"=> Saving new checkpoint\")\n",
    "        torch.save(state, filename)\n",
    "    else:\n",
    "        print(\"=> Validation Accuracy did not improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available\n"
     ]
    }
   ],
   "source": [
    "if device_ids: print(\"GPU Available\")\n",
    "else: print(\"CPU\")\n",
    "archive = zipfile.ZipFile(os.path.join(DATA_PATH, 'CompNet_data.zip'))\n",
    "\n",
    "#---------------------\n",
    "# Training Parameters\n",
    "#---------------------\n",
    "alpha = 3  # vc-loss\n",
    "beta = 3 # mix loss\n",
    "likely = 0.6 # occlusion likelihood\n",
    "lr = 1e-2 # learning rate\n",
    "batch_size = 1 # these are pseudo batches as the aspect ratio of images for CompNets is not square\n",
    "# Training setup\n",
    "vc_flag = True # train the vMF kernels\n",
    "mix_flag = True # train mixture components\n",
    "ncoord_it = 1 \t#number of epochs to train\n",
    "\n",
    "bool_mixture_model_bg = False #True: use a mixture of background models per pixel, False: use one bg model for whole image\n",
    "bool_load_pretrained_model = False\n",
    "bool_train_with_occluders = False\n",
    "dataset=\"pascal3d+\"\n",
    "\n",
    "if bool_train_with_occluders:\n",
    "    occ_levels_train = ['ZERO', 'ONE', 'FIVE', 'NINE']\n",
    "else:\n",
    "    occ_levels_train = ['ZERO']\n",
    "\n",
    "out_dir = model_save_dir + 'train_{}_a{}_b{}_vc{}_mix{}_occlikely{}_vc{}_lr_{}_{}_pretrained{}_epochs_{}_occ{}_backbone{}/'.format(\n",
    "    layer, alpha,beta, vc_flag, mix_flag, likely, vc_num, lr, dataset, bool_load_pretrained_model,ncoord_it,bool_train_with_occluders,backbone_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imgs for train 9898\n",
      "Total imgs for val 1146\n",
      "Train\n",
      "Dir: models/train_pool5_a3_b3_vcTrue_mixTrue_occlikely0.6_vc512_lr_0.01_pascal3d+_pretrainedFalse_epochs_1_occFalse_backbonevgg/, vMF_kappa: 30, alpha: 3,beta: 3, likely:0.6\n",
      "\n",
      "pretrainFalse_file\n"
     ]
    }
   ],
   "source": [
    "if backbone_type=='vgg':\n",
    "    if layer=='pool4':\n",
    "        extractor = models.vgg16(pretrained=True).features[0:24]\n",
    "    else:\n",
    "        extractor = models.vgg16(pretrained=True).features\n",
    "elif backbone_type=='resnet50' or backbone_type=='resnext':\n",
    "    extractor = resnet_feature_extractor(backbone_type, layer)\n",
    "\n",
    "extractor.cuda(device_ids[0]).eval()\n",
    "weights = getVmfKernels(dict_dir, device_ids)\n",
    "\n",
    "if bool_load_pretrained_model:\n",
    "    pretrained_file = 'PATH TO .PTH FILE HERE'\n",
    "else:\n",
    "    pretrained_file = ''\n",
    "\n",
    "occ_likely = []\n",
    "for i in range(len(categories_train)):\n",
    "    # setting the same occlusion likelihood for all classes\n",
    "    occ_likely.append(likely)\n",
    "\n",
    "# load the CompNet initialized with ML and spectral clustering\n",
    "occ_types = ['ZERO']\n",
    "mix_models = getCompositionModel(device_ids,mix_model_path,layer,categories_train,compnet_type=compnet_type)\n",
    "net = Net(extractor, weights, vMF_kappa, occ_likely, mix_models, \n",
    "          bool_mixture_bg=bool_mixture_model_bg,compnet_type=compnet_type,\n",
    "          num_mixtures=num_mixtures, vc_thresholds=cfg.MODEL.VC_THRESHOLD,\n",
    "         occ_types=occ_types)\n",
    "if bool_load_pretrained_model:\n",
    "    net.load_state_dict(torch.load(pretrained_file, map_location='cuda:{}'.format(device_ids[0]))['state_dict'])\n",
    "\n",
    "net = net.cuda(device_ids[0])\n",
    "\n",
    "train_imgs=[]\n",
    "train_masks = []\n",
    "train_labels = []\n",
    "val_imgs = []\n",
    "val_labels = []\n",
    "val_masks=[]\n",
    "zipfile_fp = os.path.join(DATA_PATH, 'CompNet_data.zip')\n",
    "archive = zipfile.ZipFile(zipfile_fp)\n",
    "\n",
    "# get training and validation images\n",
    "for occ_level in occ_levels_train:\n",
    "    if occ_level == 'ZERO':\n",
    "        occ_types = ['']\n",
    "        train_fac=0.9\n",
    "    else:\n",
    "        occ_types = ['_white', '_noise', '_texture', '']\n",
    "        train_fac=0.1\n",
    "\n",
    "    for occ_type in occ_types:\n",
    "        imgs, labels, masks = getImg(archive, 'train', categories_train, dataset, data_path, \n",
    "                                     categories, occ_level, occ_type, bool_load_occ_mask=False)\n",
    "        nimgs=len(imgs)\n",
    "        for i in range(nimgs):\n",
    "            if (random.randint(0, nimgs - 1) / nimgs) <= train_fac:\n",
    "                train_imgs.append(imgs[i])\n",
    "                train_labels.append(labels[i])\n",
    "                train_masks.append(masks[i])\n",
    "            elif not bool_train_with_occluders:\n",
    "                val_imgs.append(imgs[i])\n",
    "                val_labels.append(labels[i])\n",
    "                val_masks.append(masks[i])\n",
    "\n",
    "print('Total imgs for train ' + str(len(train_imgs)))\n",
    "print('Total imgs for val ' + str(len(val_imgs)))\n",
    "train_imgset = Imgset(archive, train_imgs,train_masks, train_labels, imgLoader,bool_square_images=False)\n",
    "\n",
    "val_imgsets = []\n",
    "if val_imgs:\n",
    "    val_imgset = Imgset(archive, val_imgs,val_masks, val_labels, imgLoader,bool_square_images=False)\n",
    "    val_imgsets.append(val_imgset)\n",
    "\n",
    "# write parameter settings into output folder\n",
    "load_flag = False\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "info = out_dir + 'config.txt'\n",
    "config_file = open(info, 'a')\n",
    "config_file.write(dataset)\n",
    "out_str = 'layer{}_a{}_b{}_vc{}_mix{}_occlikely{}_vc{}_lr{}/'.format(layer,alpha,beta,vc_flag,mix_flag,likely,vc_num,lr)\n",
    "config_file.write(out_str)\n",
    "out_str = 'Train\\nDir: {}, vMF_kappa: {}, alpha: {},beta: {}, likely:{}\\n'.format(out_dir, vMF_kappa, alpha,beta,likely)\n",
    "config_file.write(out_str)\n",
    "print(out_str)\n",
    "out_str = 'pretrain{}_file{}'.format(bool_load_pretrained_model,pretrained_file)\n",
    "print(out_str)\n",
    "config_file.write(out_str)\n",
    "config_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch1: 500/9898, Acc: 0.97, Loss: -39.04712109375 Time:32.35302233695984\n",
      "Epoch1: 1000/9898, Acc: 0.972, Loss: -39.71989453125 Time:29.01749062538147\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f6095b94a7d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m train(model=net, train_data=train_imgset, val_data=val_imgsets, epochs=ncoord_it, batch_size=batch_size,\n\u001b[0;32m----> 2\u001b[0;31m       learning_rate=lr, savedir=out_dir, alpha=alpha,beta=beta, vc_flag=vc_flag, mix_flag=mix_flag, occ_types=occ_types)\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-fe2f779db82f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data, val_data, epochs, batch_size, learning_rate, savedir, occ_types, alpha, beta, vc_flag, mix_flag)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model=net, train_data=train_imgset, val_data=val_imgsets, epochs=ncoord_it, batch_size=batch_size,\n",
    "      learning_rate=lr, savedir=out_dir, alpha=alpha,beta=beta, vc_flag=vc_flag, mix_flag=mix_flag, occ_types=occ_types)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'models/reproduce/50_epochs.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_test = Net(extractor, weights, vMF_kappa, occ_likely, \n",
    "               mix_models, bool_mixture_bg=bool_mixture_model_bg,compnet_type=compnet_type,\n",
    "               num_mixtures=num_mixtures, \n",
    "               vc_thresholds=cfg.MODEL.VC_THRESHOLD)\n",
    "net_test.load_state_dict(torch.load('models/reproduce/50_epochs.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Code.test import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_levels = ['ZERO', 'ONE', 'FIVE', 'NINE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if compnet_type=='bernoulli':\n",
    "    bool_mixture_model_bg = True\n",
    "\n",
    "if bool_load_pretrained_model:\n",
    "    if backbone_type=='vgg':\n",
    "        if layer=='pool5':\n",
    "            if dataset=='pascal3d+':\n",
    "                vc_dir = model_save_dir+'vgg_pool5_p3d+/'\n",
    "            elif dataset=='coco':\n",
    "                vc_dir = model_save_dir+'vgg_pool5_coco/'\n",
    "\n",
    "    vc_file = vc_dir + 'best.pth'\n",
    "    outdir = vc_dir\n",
    "else:\n",
    "    vc_dir = model_save_dir+'compnet_{}_{}_{}_initialization/'.format(layer,compnet_type,likely,dataset)\n",
    "    vc_path = ''\n",
    "    outdir = vc_dir\n",
    "\n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "info = outdir + 'config.txt'\n",
    "\n",
    "occ_likely = []\n",
    "for i in range(len(categories_train)):\n",
    "    occ_likely.append(likely)\n",
    "    \n",
    "net_test = Net(extractor, weights, vMF_kappa, occ_likely, \n",
    "               mix_models, bool_mixture_bg=bool_mixture_model_bg,compnet_type=compnet_type,\n",
    "               num_mixtures=num_mixtures, \n",
    "               vc_thresholds=cfg.MODEL.VC_THRESHOLD,\n",
    "              occ_types=occ_levels)\n",
    "net_test.load_state_dict(torch.load('models/reproduce/1.pth'))\n",
    "nets = [net_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4341 [00:00<07:44,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imgs for test of occ_level ZERO and occ_type  4341\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4341/4341 [01:27<00:00, 49.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class aeroplane: 0.995\n",
      "Class bicycle: 0.907\n",
      "Class bus: 0.990\n",
      "Class car: 0.982\n",
      "Class motorbike: 0.946\n",
      "Class train: 0.916\n",
      "Model Name: Occ_level:ZERO, Occ_type:, Acc:0.9705137065192352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4341 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imgs for test of occ_level ONE and occ_type  4341\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4341/4341 [02:07<00:00, 34.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class aeroplane: 0.981\n",
      "Class bicycle: 0.861\n",
      "Class bus: 0.898\n",
      "Class car: 0.960\n",
      "Class motorbike: 0.892\n",
      "Class train: 0.839\n",
      "Model Name: Occ_level:ONE, Occ_type:, Acc:0.9320433079935498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4341 [00:00<09:37,  7.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imgs for test of occ_level FIVE and occ_type  4341\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4341/4341 [02:07<00:00, 33.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class aeroplane: 0.975\n",
      "Class bicycle: 0.800\n",
      "Class bus: 0.773\n",
      "Class car: 0.879\n",
      "Class motorbike: 0.848\n",
      "Class train: 0.797\n",
      "Model Name: Occ_level:FIVE, Occ_type:, Acc:0.8682331260078323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/4341 [00:00<10:32,  6.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total imgs for test of occ_level NINE and occ_type  4341\n",
      "Testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4341/4341 [02:09<00:00, 33.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class aeroplane: 0.965\n",
      "Class bicycle: 0.725\n",
      "Class bus: 0.588\n",
      "Class car: 0.704\n",
      "Class motorbike: 0.805\n",
      "Class train: 0.742\n",
      "Model Name: Occ_level:NINE, Occ_type:, Acc:0.7505183137525916\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAADnCAYAAAC9roUQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAADKUlEQVR4nO3UMQEAIAzAMMC/5+GiHCQKenXPzAKgcV4HAPzEdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIGS6ACHTBQiZLkDIdAFCpgsQMl2AkOkChEwXIHQBcjcEy3+fc28AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############################\n",
    "# Test Loop\n",
    "############################\n",
    "for occ_level in occ_levels:\n",
    "\n",
    "    if occ_level == 'ZERO':\n",
    "        occ_types = ['']\n",
    "    else:\n",
    "        if dataset=='pascal3d+':\n",
    "            occ_types = ['']#['_white','_noise', '_texture', '']\n",
    "        elif dataset=='coco':\n",
    "            occ_types = ['']\n",
    "\n",
    "    for index, occ_type in enumerate(occ_types):\n",
    "        # load images\n",
    "        data_path = ''\n",
    "        test_imgs, test_labels, masks = getImg(archive, 'test', categories_train, dataset, data_path, \n",
    "                                     categories, occ_level, occ_type, bool_load_occ_mask=True)\n",
    "        print('Total imgs for test of occ_level {} and occ_type {} '.format(occ_level, occ_type) + str(len(test_imgs)))\n",
    "        # get image loader\n",
    "        test_imgset = Imgset(archive, test_imgs, masks, test_labels, imgLoader, bool_square_images=False)\n",
    "        # compute test accuracy\n",
    "        acc,scores = test(models=nets, test_data=test_imgset, batch_size=1)\n",
    "        out_str = 'Model Name: Occ_level:{}, Occ_type:{}, Acc:{}'.format(occ_level, occ_type, acc)\n",
    "        print(out_str)\n",
    "        \n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        last_idx = 5\n",
    "        test_loader = DataLoader(test_imgset, batch_size=1, shuffle=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, data in enumerate(test_loader):\n",
    "                input_img, mask, label = data\n",
    "                if device_ids:\n",
    "                    input_img = input_img.cuda(device_ids[0])\n",
    "        #         c_label = label.numpy()\n",
    "\n",
    "        #         output, *_ = model(input_img)\n",
    "        #         out = output.cpu().numpy().argmax(-1)[0]\n",
    "        #         results.append(out)\n",
    "        #         pred_class = categories_train[out]\n",
    "        #         print('\\nImage {} classified as {}'.format(img_name,pred_class))\n",
    "                #localize occluder\n",
    "\n",
    "                score, occ_maps, part_scores = net.get_occlusion(input_img, label)\n",
    "                occ_map = occ_maps[0].detach().cpu().numpy()\n",
    "                occ_map = cv2.medianBlur(occ_map.astype(np.float32), 3)\n",
    "                occ_img = visualize_response_map(occ_map, tit='',cbarmax=10, show=False)\n",
    "\n",
    "                # concatenate original image and occluder map\n",
    "                img_orig = np.array(Image.open(archive.open(test_imgs[i])))\n",
    "        #         img_orig = np.array(input_img.squeeze().cpu().permute(1, 2, 0))\n",
    "        #         img_orig = img_orig * std + mean\n",
    "        #         img_orig = (255*(img_orig - np.min(img_orig))/np.ptp(img_orig).astype(int)).astype('uint8')\n",
    "        #         faco = img_orig.shape[0] / occ_img.shape[0]\n",
    "        #         occ_img_s = cv2.resize(occ_img, (int(occ_img.shape[1] * faco), img_orig.shape[0]))\n",
    "                occ_img_s = Image.fromarray(occ_img).resize((img_orig.shape[1], img_orig.shape[0]))\n",
    "        #         occ_img_s = cv2.resize(occ_img, (img_orig.shape[1], img_orig.shape[0]))\n",
    "        #         occ_img_s = np.array(transforms.ToTensor()(occ_img_s).permute(1, 2, 0)) * std + mean\n",
    "        #         occ_img_s = occ_img_s.astype('float64')\n",
    "        #         occ_img_s = (occ_img_s - np.min(occ_img_s))/np.ptp(occ_img_s)\n",
    "                out_name = f'results/reproduce/without_occluder_2/{occ_level}_{i:04d}.png'\n",
    "                canvas = np.concatenate((img_orig, occ_img_s), axis=1)\n",
    "        #         Image.fromarray(canvas).save(out_name)\n",
    "        #         cv2.imwrite(out_name, canvas)\n",
    "#                 plt.imshow(canvas)\n",
    "                plt.axis('off')\n",
    "                plt.imsave(out_name, canvas)\n",
    "\n",
    "                if i == last_idx: break\n",
    "        # print(f'Accuracy: {accuracy_score(test_labels, results)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "last_idx = 5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in tqdm(enumerate(test_loader)):\n",
    "        input_img, mask, label = data\n",
    "        if device_ids:\n",
    "            input_img = input_img.cuda(device_ids[0])\n",
    "#         c_label = label.numpy()\n",
    "        \n",
    "#         output, *_ = model(input_img)\n",
    "#         out = output.cpu().numpy().argmax(-1)[0]\n",
    "#         results.append(out)\n",
    "#         pred_class = categories_train[out]\n",
    "#         print('\\nImage {} classified as {}'.format(img_name,pred_class))\n",
    "        #localize occluder\n",
    "    \n",
    "        score, occ_maps, part_scores = net.get_occlusion(input_img, label)\n",
    "        occ_map = occ_maps[0].detach().cpu().numpy()\n",
    "        occ_map = cv2.medianBlur(occ_map.astype(np.float32), 3)\n",
    "        occ_img = visualize_response_map(occ_map, tit='',cbarmax=10, show=False)\n",
    "        \n",
    "        # concatenate original image and occluder map\n",
    "        img_orig = np.array(Image.open(archive.open(test_images[i])))\n",
    "#         img_orig = np.array(input_img.squeeze().cpu().permute(1, 2, 0))\n",
    "#         img_orig = img_orig * std + mean\n",
    "#         img_orig = (255*(img_orig - np.min(img_orig))/np.ptp(img_orig).astype(int)).astype('uint8')\n",
    "#         faco = img_orig.shape[0] / occ_img.shape[0]\n",
    "#         occ_img_s = cv2.resize(occ_img, (int(occ_img.shape[1] * faco), img_orig.shape[0]))\n",
    "        occ_img_s = Image.fromarray(occ_img).resize((img_orig.shape[1], img_orig.shape[0]))\n",
    "#         occ_img_s = cv2.resize(occ_img, (img_orig.shape[1], img_orig.shape[0]))\n",
    "#         occ_img_s = np.array(transforms.ToTensor()(occ_img_s).permute(1, 2, 0)) * std + mean\n",
    "#         occ_img_s = occ_img_s.astype('float64')\n",
    "#         occ_img_s = (occ_img_s - np.min(occ_img_s))/np.ptp(occ_img_s)\n",
    "        out_name = f'results/reproduce/without_occluder/{i:04d}.png'\n",
    "        canvas = np.concatenate((img_orig, occ_img_s), axis=1)\n",
    "#         Image.fromarray(canvas).save(out_name)\n",
    "#         cv2.imwrite(out_name, canvas)\n",
    "        plt.imshow(canvas)\n",
    "        plt.axis('off')\n",
    "        plt.imsave(out_name, canvas)\n",
    "        \n",
    "        if i == last_idx: break\n",
    "# print(f'Accuracy: {accuracy_score(test_labels, results)}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CompositionalNets.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
