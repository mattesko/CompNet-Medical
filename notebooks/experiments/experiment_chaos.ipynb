{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "\n",
    "from comet_ml import Experiment\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import kornia.augmentation as K\n",
    "\n",
    "from src.dataset import ClassificationDataset, NormalizeInstance, get_image_pair_filepaths, Resize, Chaos2DSegmentationDataset\n",
    "from src.models import UNet\n",
    "from src.metrics import dice_loss, dice_score\n",
    "from src.utils import create_canvas\n",
    "from src.train import train_one_epoch, validate\n",
    "import src.config\n",
    "from src.config import directories, Directories\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 # 0: off, 2: on for all modules\n",
    "# os.chdir('CompositionalNets/')\n",
    "# sys.path.append('/project/6052161/mattlk/workplace/CompNet')\n",
    "\n",
    "# Change the below directory depending on where the CHAOS dataset is stored\n",
    "data_dir = src.config.directories['chaos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment = Experiment(api_key=\"P5seMqEJjqZ8mDA7QYSuK3yUJ\",\n",
    "#                         project_name=\"chaos-liver-segmentation\",\n",
    "#                         workspace=\"matthew42\", auto_metric_logging=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train U-Net on CHAOS for Liver Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_normalize(x):\n",
    "    x = np.stack((x, x, x), axis=2)\n",
    "    x = (x - x.min()) / (x.max() - x.min() + 1e-12)\n",
    "    x = x.astype(np.float32)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 16,\n",
    "    \"split_train_val\": 0.8,\n",
    "    \"epochs\": 45,\n",
    "    \"use_dice_loss\": False,\n",
    "    \"cache\": False,\n",
    "    \"random_seed\": 42,\n",
    "    \"shuffle_data\": True,\n",
    "    \"scheduler\": \"StepLR\",\n",
    "    \"step_size\": 15,\n",
    "    \"gamma\": 0.75,\n",
    "    \"threshold\": 0.9,\n",
    "    \"pretrained\": True,\n",
    "}\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if is_cuda_available else \"cpu\")\n",
    "input_images_dtype = torch.double\n",
    "targets_dtype = torch.long\n",
    "if is_cuda_available: torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "size = (256, 256)\n",
    "crop = (224, 224)\n",
    "train_input_transform = transforms.Compose([\n",
    "    stack_normalize,\n",
    "    transforms.ToTensor(),\n",
    "    Resize(size),\n",
    "    K.CenterCrop(crop),\n",
    "#     K.RandomAffine(degrees=(-5, 5), translate=(0.05, 0.05), scale=(0.75, 1.25)),\n",
    "    transforms.Lambda(lambda x: x.squeeze()),\n",
    "])\n",
    "\n",
    "train_target_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.astype(np.uint8)),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(size),\n",
    "    transforms.CenterCrop(crop),\n",
    "    transforms.ToTensor(),\n",
    "#     K.RandomAffine(degrees=(-5, 5), translate=(0.05, 0.05), scale=(0.75, 1.25)),\n",
    "    transforms.Lambda(lambda x: x.squeeze()),\n",
    "    transforms.Lambda(lambda x: x*255),\n",
    "    transforms.Lambda(lambda x: x.long()),\n",
    "])\n",
    "\n",
    "val_input_transform = transforms.Compose([\n",
    "    stack_normalize,\n",
    "    transforms.ToTensor(),\n",
    "    Resize(size),\n",
    "    K.CenterCrop(crop),\n",
    "    transforms.Lambda(lambda x: x.squeeze()),\n",
    "])\n",
    "val_target_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.astype(np.uint8)),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(size),\n",
    "    transforms.CenterCrop(crop),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x*255),\n",
    "    transforms.Lambda(lambda x: x.long()),\n",
    "])\n",
    "\n",
    "data_dir = Directories.CHAOS\n",
    "\n",
    "image_pair_filepaths = get_image_pair_filepaths(data_dir)[:]\n",
    "train_filepaths, val_filepaths = train_test_split(image_pair_filepaths,\n",
    "                                                  train_size=params['split_train_val'],\n",
    "                                                  random_state=params['random_seed'],\n",
    "                                                  shuffle=params[\"shuffle_data\"])\n",
    "\n",
    "train_dataset = Chaos2DSegmentationDataset(train_filepaths, \n",
    "                                           input_transform=train_input_transform,\n",
    "                                           target_transform=train_target_transform,\n",
    "                                           cache=params['cache'])\n",
    "\n",
    "val_dataset = Chaos2DSegmentationDataset(val_filepaths,\n",
    "                                         input_transform=val_input_transform,\n",
    "                                         target_transform=val_target_transform,\n",
    "                                         cache=params['cache'])\n",
    "\n",
    "num_train, num_val = len(train_dataset), len(val_dataset)\n",
    "params['num_samples'] = num_train + num_val\n",
    "params['input_transform'] = train_input_transform.__str__()\n",
    "params['target_transform'] = train_target_transform.__str__()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input type float32 is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-5df4f945b5f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m img = create_canvas(image, target, show=False,\n\u001b[1;32m      5\u001b[0m                            title1='Example Input', title2='Example Target')\n",
      "\u001b[0;32m/project/6052161/mattlk/workplace/CompNet-Medical/src/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_transform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0minput_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_seed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \"\"\"\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input type {} is not supported'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input type float32 is not supported"
     ]
    }
   ],
   "source": [
    "image, target = train_dataset[42]\n",
    "image = image.clone().permute(1, 2, 0).numpy()\n",
    "target = target.clone().numpy()\n",
    "img = create_canvas(image, target, show=False,\n",
    "                           title1='Example Input', title2='Example Target')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2b4a284fc9b0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, dataloader, epoch, device=None, input_dtype=torch.double,\n",
    "    target_dtype=torch.long, use_dice_loss=False, experiment=None,\n",
    "    batch_freq=50, epoch_freq=25, threshold=0.5, **kwargs):\n",
    "    from src.metrics import jaccard_score, dice_score\n",
    "    import torch.nn.functional as F\n",
    "    \"\"\"Gather validation metrics (Dice, Jaccard) on neural network\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    dice_mean = torch.zeros((1), device=device)\n",
    "    jaccard_mean = torch.zeros((1), device=device)\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "\n",
    "        input_images, targets = data\n",
    "\n",
    "        if device:\n",
    "            input_images = input_images.to(device, input_dtype)\n",
    "            targets = targets.to(device, target_dtype)\n",
    "\n",
    "        outputs = net(input_images)\n",
    "\n",
    "        if use_dice_loss:\n",
    "            outputs = F.log_softmax(outputs, dim=1)\n",
    "        else:\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            outputs = F.threshold(outputs[:, 1, :, :].unsqueeze(dim=1), threshold, 0)\n",
    "            outputs = torch.round(outputs)\n",
    "\n",
    "        score = dice_score(outputs, targets)\n",
    "        dice_mean = dice_mean + (score - dice_mean) / (i + 1)\n",
    "        score = jaccard_score(outputs, targets)\n",
    "        jaccard_mean = jaccard_mean + (score - jaccard_mean) / (i + 1)\n",
    "\n",
    "#         outputs, targets = outputs.data.cpu().numpy()*255, targets.data.cpu().numpy()*255\n",
    "#         for idx, (out, gt) in enumerate(zip(outputs, targets)):\n",
    "#             img = create_canvas(out, gt, show=False)\n",
    "#             plt.figure(figsize=(10, 10))\n",
    "#             plt.imshow(img)\n",
    "\n",
    "    return dice_mean.item(), jaccard_mean.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures)\u001b[0m\n\u001b[1;32m   2986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstore_history\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2987\u001b[0m             self.history_manager.store_inputs(self.execution_count,\n\u001b[0;32m-> 2988\u001b[0;31m                                               cell, raw_cell)\n\u001b[0m\u001b[1;32m   2989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2990\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/IPython/core/history.py\u001b[0m in \u001b[0;36mstore_inputs\u001b[0;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_hist_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_input_cache_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_input_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;31m# Trigger to flush cache and write to DB.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# with experiment.train():\n",
    "num_accumulated_steps = 1\n",
    "\n",
    "print(f'Number of training images:\\t{num_train}\\nNumber of validation images:\\t{num_val}')\n",
    "for epoch in range(params['epochs']):\n",
    "\n",
    "    unet, running_loss = train_one_epoch(unet, train_dataloader, optimizer,\n",
    "                                         criterion, device=device,\n",
    "                                         num_accumulated_steps=num_accumulated_steps, \n",
    "                                         **params)\n",
    "\n",
    "    if params['use_dice_loss']:\n",
    "        print(f'[Epoch {epoch+1:03d} Training]\\tDice Loss:\\t\\t{running_loss:.4f}')\n",
    "    else:\n",
    "        print(f'[Epoch {epoch+1:03d} Training]\\tCross-Entropy Loss:\\t{running_loss:.4f}')\n",
    "#     experiment.log_metric(\"Running Loss\", running_loss, epoch=epoch, step=epoch, include_context=False)\n",
    "\n",
    "    f1_mean, jaccard_mean = validate(unet, val_dataloader, epoch, device,\n",
    "#                                      experiment=experiment, batch_freq=25,\n",
    "                                     experiment=None, batch_freq=25,\n",
    "                                     epoch_freq=25, **params)\n",
    "\n",
    "    if params['scheduler'] == 'ReduceLROnPlateau':\n",
    "        scheduler.step(f1_mean)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "    print(f'[Epoch {epoch+1:03d} Validation]\\tAverage F1 Score:\\t{f1_mean:.4f}\\tAverage Jaccard/IoU:\\t{jaccard_mean:.4f}\\n')\n",
    "\n",
    "#     experiment.log_metric('Validation Average F1 Score', f1_mean,\n",
    "#                           epoch=epoch, include_context=False)\n",
    "#     experiment.log_metric('Validation Average Jaccard/IoU', jaccard_mean,\n",
    "#                           epoch=epoch, include_context=False)\n",
    "\n",
    "# torch.save(unet.state_dict(), 'unet.pth')\n",
    "# experiment.log_asset('unet.pth', copy_to_tmp=False)\n",
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
