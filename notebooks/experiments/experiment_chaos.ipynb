{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pdb\n",
    "\n",
    "from comet_ml import Experiment\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import kornia.augmentation as K\n",
    "\n",
    "from src.dataset import ClassificationDataset, NormalizeInstance, get_image_pair_filepaths\n",
    "from src.models import UNet\n",
    "from src.metrics import dice_loss, dice_score\n",
    "from src.utils import create_canvas\n",
    "from src.train import train_one_epoch, validate\n",
    "import src.config\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 # 0: off, 2: on for all modules\n",
    "# os.chdir('CompositionalNets/')\n",
    "# sys.path.append('/project/6052161/mattlk/workplace/CompNet')\n",
    "\n",
    "# Change the below directory depending on where the CHAOS dataset is stored\n",
    "data_dir = src.config.directories['chaos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment = Experiment(api_key=\"P5seMqEJjqZ8mDA7QYSuK3yUJ\",\n",
    "#                         project_name=\"chaos-liver-segmentation\",\n",
    "#                         workspace=\"matthew42\", auto_metric_logging=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train U-Net on CHAOS for Liver Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/project/6052161/mattlk/workplace/CompNet-Medical/src/models.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dice, in_channels, pretrained, num_classes, bias)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv6_up\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvTranspose2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv6_input\u001b[0m \u001b[0;34m=\u001b[0m      \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv6\u001b[0m \u001b[0;34m=\u001b[0m            \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv7_up\u001b[0m \u001b[0;34m=\u001b[0m         \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConvTranspose2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, groups, bias, padding_mode)\u001b[0m\n\u001b[1;32m    336\u001b[0m         super(Conv2d, self).__init__(\n\u001b[1;32m    337\u001b[0m             \u001b[0min_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             False, _pair(0), groups, bias, padding_mode)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mreset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkaiming_uniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mfan_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_calculate_fan_in_and_fan_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/torch/nn/init.py\u001b[0m in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m  \u001b[0;31m# Calculate uniform bounds from standard deviation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mbound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    \"lr\": 0.0001,\n",
    "    \"batch_size\": 16,\n",
    "    \"split_train_val\": 0.8,\n",
    "    \"epochs\": 45,\n",
    "    \"use_dice_loss\": False,\n",
    "    \"cache\": True,\n",
    "    \"random_seed\": 42,\n",
    "    \"shuffle_data\": True,\n",
    "    \"scheduler\": \"StepLR\",\n",
    "    \"step_size\": 15,\n",
    "    \"gamma\": 0.75,\n",
    "    \"threshold\": 0.5,\n",
    "    \"pretrained\": True,\n",
    "}\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if is_cuda_available else \"cpu\")\n",
    "input_images_dtype = torch.double\n",
    "targets_dtype = torch.long\n",
    "if is_cuda_available: torch.cuda.empty_cache()\n",
    "\n",
    "cache_input_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Grayscale(3),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "cache_target_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.astype(np.uint8)),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Lambda(lambda x: x*255),\n",
    "#     transforms.Lambda(lambda x: x.long()),\n",
    "])\n",
    "\n",
    "# input_transform = transforms.Compose([\n",
    "#     K.RandomAffine(0, shear=(-5, 5)),\n",
    "#     K.RandomHorizontalFlip(),\n",
    "#     transforms.Lambda(lambda x: x.squeeze()),\n",
    "# ])\n",
    "input_transform = None\n",
    "\n",
    "target_transform = transforms.Compose([\n",
    "#     K.RandomAffine(0, shear=(-5, 5)),\n",
    "#     K.RandomHorizontalFlip(),\n",
    "#     transforms.Lambda(lambda x: x.squeeze()),\n",
    "    transforms.Lambda(lambda x: x*255),\n",
    "    transforms.Lambda(lambda x: x.long()),\n",
    "])\n",
    "target_transform = None\n",
    "\n",
    "data_dir = src.config.directories['chaos']\n",
    "hdf5_path = os.path.join(data_dir, 'train.hdf5')\n",
    "with h5py.File(hdf5_path, 'r') as hf:\n",
    "    images, targets = hf['images'][:8], hf['masks'][:8]\n",
    "\n",
    "images = [cache_input_transform(im) for im in images]\n",
    "targets = [cache_target_transform(t) for t in targets]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, targets, \n",
    "                                                    train_size=params['split_train_val'],\n",
    "                                                    random_state=params['random_seed'],\n",
    "                                                    shuffle=params[\"shuffle_data\"])\n",
    "X_test = X_train\n",
    "y_test = y_train\n",
    "\n",
    "train_dataset = ClassificationDataset(X_train, y_train, \n",
    "                                      input_transform, target_transform)\n",
    "val_dataset = ClassificationDataset(X_test, y_test,\n",
    "                                    target_transform=transforms.Compose(\n",
    "                                        [transforms.Lambda(lambda x: x*255), transforms.Lambda(lambda x: x.long())]))\n",
    "\n",
    "num_train, num_val = len(train_dataset), len(val_dataset)\n",
    "params['num_samples'] = num_train + num_val\n",
    "params['target_transform'] = target_transform.__str__()\n",
    "params['input_transform'] = input_transform.__str__()\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=params['batch_size'])\n",
    "\n",
    "# Instantiate model, optimizer, and criterion\n",
    "unet = UNet(dice=params['use_dice_loss'], pretrained=params['pretrained'])\n",
    "if is_cuda_available: unet = unet.to(device, dtype=input_images_dtype)\n",
    "\n",
    "optimizer = optim.Adam(unet.parameters(), lr=params['lr'])\n",
    "if params['scheduler'] == 'StepLR':\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer,\n",
    "    step_size=params['step_size'], gamma=params['gamma'])\n",
    "elif params['scheduler'] == 'ReduceLROnPlateau':\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# cross-entropy loss: weighting of negative vs positive pixels\n",
    "loss_weight = torch.DoubleTensor([0.01, 0.99])\n",
    "if is_cuda_available: loss_weight = loss_weight.to(device)\n",
    "criterion = dice_loss if params['use_dice_loss'] else CrossEntropyLoss(weight=loss_weight,\n",
    "reduction='mean')\n",
    "\n",
    "image, target = train_dataset[0]\n",
    "image = image.clone().permute(1, 2, 0).numpy()\n",
    "target = target.clone().numpy()\n",
    "img = create_canvas(image, target, show=False,\n",
    "                   title1='Example Input', title2='Example Target')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(net, dataloader, epoch, device=None, input_dtype=torch.double,\n",
    "    target_dtype=torch.long, use_dice_loss=False, experiment=None,\n",
    "    batch_freq=50, epoch_freq=25, threshold=0.5, **kwargs):\n",
    "    from src.metrics import jaccard_score, dice_score\n",
    "    import torch.nn.functional as F\n",
    "    \"\"\"Gather validation metrics (Dice, Jaccard) on neural network\n",
    "    \"\"\"\n",
    "    net.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    dice_mean = torch.zeros((1), device=device)\n",
    "    jaccard_mean = torch.zeros((1), device=device)\n",
    "\n",
    "    for i, data in enumerate(dataloader):\n",
    "\n",
    "        input_images, targets = data\n",
    "\n",
    "        if device:\n",
    "            input_images = input_images.to(device, input_dtype)\n",
    "            targets = targets.to(device, target_dtype)\n",
    "\n",
    "        outputs = net(input_images)\n",
    "\n",
    "        if use_dice_loss:\n",
    "            outputs = F.log_softmax(outputs, dim=1)\n",
    "        else:\n",
    "            outputs = F.softmax(outputs, dim=1)\n",
    "            outputs = F.threshold(outputs[:, 1, :, :].unsqueeze(dim=1), threshold, 0)\n",
    "            outputs = torch.round(outputs)\n",
    "\n",
    "        score = dice_score(outputs, targets)\n",
    "        dice_mean = dice_mean + (score - dice_mean) / (i + 1)\n",
    "        score = jaccard_score(outputs, targets)\n",
    "        jaccard_mean = jaccard_mean + (score - jaccard_mean) / (i + 1)\n",
    "\n",
    "#         outputs, targets = outputs.data.cpu().numpy()*255, targets.data.cpu().numpy()*255\n",
    "#         for idx, (out, gt) in enumerate(zip(outputs, targets)):\n",
    "#             img = create_canvas(out, gt, show=False)\n",
    "#             plt.figure(figsize=(10, 10))\n",
    "#             plt.imshow(img)\n",
    "\n",
    "    return dice_mean.item(), jaccard_mean.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures)\u001b[0m\n\u001b[1;32m   2986\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstore_history\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2987\u001b[0m             self.history_manager.store_inputs(self.execution_count,\n\u001b[0;32m-> 2988\u001b[0;31m                                               cell, raw_cell)\n\u001b[0m\u001b[1;32m   2989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2990\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_cell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/IPython/core/history.py\u001b[0m in \u001b[0;36mstore_inputs\u001b[0;34m(self, line_num, source, source_raw)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_hist_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_input_cache_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdb_input_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;31m# Trigger to flush cache and write to DB.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# with experiment.train():\n",
    "num_accumulated_steps = 1\n",
    "\n",
    "print(f'Number of training images:\\t{num_train}\\nNumber of validation images:\\t{num_val}')\n",
    "for epoch in range(params['epochs']):\n",
    "\n",
    "    unet, running_loss = train_one_epoch(unet, train_dataloader, optimizer,\n",
    "                                         criterion, device=device,\n",
    "                                         num_accumulated_steps=num_accumulated_steps, \n",
    "                                         **params)\n",
    "\n",
    "    if params['use_dice_loss']:\n",
    "        print(f'[Epoch {epoch+1:03d} Training]\\tDice Loss:\\t\\t{running_loss:.4f}')\n",
    "    else:\n",
    "        print(f'[Epoch {epoch+1:03d} Training]\\tCross-Entropy Loss:\\t{running_loss:.4f}')\n",
    "#     experiment.log_metric(\"Running Loss\", running_loss, epoch=epoch, step=epoch, include_context=False)\n",
    "\n",
    "    f1_mean, jaccard_mean = validate(unet, val_dataloader, epoch, device,\n",
    "#                                      experiment=experiment, batch_freq=25,\n",
    "                                     experiment=None, batch_freq=25,\n",
    "                                     epoch_freq=25, **params)\n",
    "\n",
    "    if params['scheduler'] == 'ReduceLROnPlateau':\n",
    "        scheduler.step(f1_mean)\n",
    "    else:\n",
    "        scheduler.step()\n",
    "    print(f'[Epoch {epoch+1:03d} Validation]\\tAverage F1 Score:\\t{f1_mean:.4f}\\tAverage Jaccard/IoU:\\t{jaccard_mean:.4f}\\n')\n",
    "\n",
    "#     experiment.log_metric('Validation Average F1 Score', f1_mean,\n",
    "#                           epoch=epoch, include_context=False)\n",
    "#     experiment.log_metric('Validation Average Jaccard/IoU', jaccard_mean,\n",
    "#                           epoch=epoch, include_context=False)\n",
    "\n",
    "# torch.save(unet.state_dict(), 'unet.pth')\n",
    "# experiment.log_asset('unet.pth', copy_to_tmp=False)\n",
    "# experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
