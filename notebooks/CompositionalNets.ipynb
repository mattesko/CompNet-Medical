{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImg(archive, mode, categories, dataset, data_path, \n",
    "           cat_test=None, occ_level='ZERO', occ_type=None, bool_load_occ_mask = False):\n",
    "\n",
    "    if mode == 'train':\n",
    "        train_imgs = []\n",
    "        train_labels = []\n",
    "        train_masks = []\n",
    "        for category in categories:\n",
    "            if dataset == 'pascal3d+':\n",
    "                if occ_level == 'ZERO':\n",
    "                    filelist = 'pascal3d+_occ/' + category + '_imagenet_train' + '.txt'\n",
    "                    img_dir = 'pascal3d+_occ/TRAINING_DATA/' + category + '_imagenet'\n",
    "            elif dataset == 'coco':\n",
    "                if occ_level == 'ZERO':\n",
    "                    img_dir = 'coco_occ/{}_zero'.format(category)\n",
    "                    filelist = 'coco_occ/{}_{}_train.txt'.format(category, occ_level)\n",
    "\n",
    "            with archive.open(filelist, 'r') as fh:\n",
    "                contents = fh.readlines()\n",
    "            img_list = [cc.strip().decode('ascii') for cc in contents]\n",
    "            label = categories.index(category)\n",
    "            for img_path in img_list:\n",
    "                if dataset=='coco':\n",
    "                    if occ_level == 'ZERO':\n",
    "                        img = img_dir + '/' + img_path + '.jpg'\n",
    "                    else:\n",
    "                        img = img_dir + '/' + img_path + '.JPEG'\n",
    "                else:\n",
    "                    img = img_dir + '/' + img_path + '.JPEG'\n",
    "                occ_img1 = []\n",
    "                occ_img2 = []\n",
    "                train_imgs.append(img)\n",
    "                train_labels.append(label)\n",
    "                train_masks.append([occ_img1,occ_img2])\n",
    "        \n",
    "        return train_imgs, train_labels, train_masks\n",
    "\n",
    "    else:\n",
    "        test_imgs = []\n",
    "        test_labels = []\n",
    "        occ_imgs = []\n",
    "        for category in cat_test:\n",
    "            if dataset == 'pascal3d+':\n",
    "                filelist = data_path + 'pascal3d+_occ/' + category + '_imagenet_occ.txt'\n",
    "                img_dir = data_path + 'pascal3d+_occ/' + category + 'LEVEL' + occ_level\n",
    "                if bool_load_occ_mask:\n",
    "                    if  occ_type=='':\n",
    "                        occ_mask_dir = 'pascal3d+_occ/' + category + 'LEVEL' + occ_level+'_mask_object'\n",
    "                    else:\n",
    "                        occ_mask_dir = 'pascal3d+_occ/' + category + 'LEVEL' + occ_level+'_mask'\n",
    "                    occ_mask_dir_obj = 'pascal3d+_occ/0_old_masks/'+category+'_imagenet_occludee_mask/'\n",
    "            elif dataset == 'coco':\n",
    "                if occ_level == 'ZERO':\n",
    "                    img_dir = 'coco_occ/{}_zero'.format(category)\n",
    "                    filelist = 'coco_occ/{}_{}_test.txt'.format(category, occ_level)\n",
    "                else:\n",
    "                    img_dir = 'coco_occ/{}_occ'.format(category)\n",
    "                    filelist = 'coco_occ/{}_{}.txt'.format(category, occ_level)\n",
    "\n",
    "#             if os.path.exists(filelist):\n",
    "            with archive.open(filelist, 'r') as fh:\n",
    "                contents = fh.readlines()\n",
    "            img_list = [cc.strip().decode('ascii') for cc in contents]\n",
    "            label = categories.index(category)\n",
    "            for img_path in img_list:\n",
    "                if dataset != 'coco':\n",
    "                    if occ_level=='ZERO':\n",
    "                        img = img_dir + occ_type + '/' + img_path[:-2] + '.JPEG'\n",
    "                        occ_img1 = []\n",
    "                        occ_img2 = []\n",
    "                    else:\n",
    "                        img = img_dir + occ_type + '/' + img_path + '.JPEG'\n",
    "                        if bool_load_occ_mask:\n",
    "                            occ_img1 = occ_mask_dir + '/' + img_path + '.JPEG'\n",
    "                            occ_img2 = occ_mask_dir_obj + '/' + img_path + '.png'\n",
    "                        else:\n",
    "                            occ_img1 = []\n",
    "                            occ_img2 = []\n",
    "\n",
    "                else:\n",
    "                    img = img_dir + occ_type + '/' + img_path + '.jpg'\n",
    "                    occ_img1 = []\n",
    "                    occ_img2 = []\n",
    "\n",
    "                test_imgs.append(img)\n",
    "                test_labels.append(label)\n",
    "                occ_imgs.append([occ_img1,occ_img2])\n",
    "#             else:\n",
    "#                 print('FILELIST NOT FOUND: {}'.format(filelist))\n",
    "        return test_imgs, test_labels, occ_imgs\n",
    "\n",
    "\n",
    "def imgLoader(archive, img_path,mask_path,bool_resize_images=True,bool_square_images=False):\n",
    "    \n",
    "    archive_img_path = archive.open(img_path)\n",
    "    input_image = Image.open(archive_img_path)\n",
    "    if bool_resize_images:\n",
    "        if bool_square_images:\n",
    "            input_image.resize((224,224),Image.ANTIALIAS)\n",
    "        else:\n",
    "            sz=input_image.size\n",
    "            min_size = np.min(sz)\n",
    "            if min_size!=224:\n",
    "                input_image = input_image.resize((np.asarray(sz) * (224 / min_size)).astype(int),Image.ANTIALIAS)\n",
    "    preprocess =  transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "    img = preprocess(input_image)\n",
    "\n",
    "    if mask_path[0]:\n",
    "        f = archive.open(mask_path[0])\n",
    "        mask1 = np.array(Image.open(f))\n",
    "        f.close()\n",
    "        mask1 = myresize(mask1, 224, 'short')\n",
    "        try:\n",
    "            mask2 = cv2.imread(mask_path[1])[:, :, 0]\n",
    "            mask2 = mask2[:mask1.shape[0], :mask1.shape[1]]\n",
    "        except:\n",
    "            mask = mask1\n",
    "        try:\n",
    "            mask = ((mask1 == 255) * (mask2 == 255)).astype(np.float)\n",
    "        except:\n",
    "            mask = mask1\n",
    "    else:\n",
    "        mask = np.ones((img.shape[0], img.shape[1])) * 255.0\n",
    "\n",
    "    mask = torch.from_numpy(mask)\n",
    "    return img,mask\n",
    "\n",
    "\n",
    "class Imgset():\n",
    "    def __init__(self, archive, imgs, masks, labels, loader,bool_square_images=False):\n",
    "        self.archive = archive\n",
    "        self.images = imgs\n",
    "        self.masks \t= masks\n",
    "        self.labels = labels\n",
    "        self.loader = loader\n",
    "        self.bool_square_images = bool_square_images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        fn = self.images[index]\n",
    "        label = self.labels[index]\n",
    "        mask = self.masks[index]\n",
    "        img,mask = self.loader(self.archive,fn,mask,bool_resize_images=True,bool_square_images=self.bool_square_images)\n",
    "        return img, mask, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "def save_checkpoint(state, filename, is_best):\n",
    "    if is_best:\n",
    "        print(\"=> Saving new checkpoint\")\n",
    "        torch.save(state, filename)\n",
    "    else:\n",
    "        print(\"=> Validation Accuracy did not improve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import cv2\n",
    "\n",
    "from CompositionalNets.Code.vMFMM import *\n",
    "from CompositionalNets.Initialization_Code.config_initialization import dataset, categories, vc_num, data_path, cat_test, device_ids, Astride, Apad, Arf, vMF_kappa, layer,init_path, nn_type, dict_dir, offset, extractor\n",
    "from CompositionalNets.Code.helpers import myresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n"
     ]
    }
   ],
   "source": [
    "# Number of images to train on per category\n",
    "# Clustering ignored after this threshold is met\n",
    "img_per_cat = 1000\n",
    "\n",
    "# Number of feature vectors to sample from each image's feature map\n",
    "samp_size_per_img = 20 \n",
    "\n",
    "imgs_par_cat = np.zeros(len(categories))\n",
    "bool_load_existing_cluster = False\n",
    "bins = 4\n",
    "\n",
    "archive = zipfile.ZipFile(os.path.join(data_path, 'CompNet_data.zip'))\n",
    "occ_level = 'ZERO'\n",
    "occ_type = ''\n",
    "imgs, labels, masks = getImg(archive, 'train', categories, dataset, data_path, cat_test, occ_level, occ_type, bool_load_occ_mask=False)\n",
    "imgset = Imgset(archive, imgs, masks, labels, imgLoader, bool_square_images=False)\n",
    "data_loader = DataLoader(dataset=imgset, batch_size=1, shuffle=False)\n",
    "nimgs = len(imgs)\n",
    "\n",
    "loc_set = []\n",
    "feat_set = []\n",
    "nfeats = 0\n",
    "for ii,data in enumerate(data_loader):\n",
    "    input, mask, label = data\n",
    "    if np.mod(ii,500)==0:\n",
    "        print('{} / {}'.format(ii,len(imgs)))\n",
    "\n",
    "    fname = imgs[ii]\n",
    "    category = labels[ii]\n",
    "\n",
    "    if imgs_par_cat[label]<img_per_cat:\n",
    "        with torch.no_grad():\n",
    "            tmp = extractor(input.cuda(device_ids[0]))[0].detach().cpu().numpy()\n",
    "        height, width = tmp.shape[1:3]\n",
    "        img = cv2.imread(imgs[ii])\n",
    "\n",
    "        # Crop image by some offset\n",
    "        tmp = tmp[:,offset:height - offset, offset:width - offset]\n",
    "\n",
    "        # Flatten image at each channel\n",
    "        gtmp = tmp.reshape(tmp.shape[0], -1)\n",
    "        if gtmp.shape[1] >= samp_size_per_img:\n",
    "            rand_idx = np.random.permutation(gtmp.shape[1])[:samp_size_per_img]\n",
    "        else:\n",
    "            rand_idx = np.random.permutation(gtmp.shape[1])[:samp_size_per_img - gtmp.shape[1]]\n",
    "            #rand_idx = np.append(range(gtmp.shape[1]), rand_idx)\n",
    "        tmp_feats = gtmp[:, rand_idx].T\n",
    "\n",
    "        cnt = 0\n",
    "        for rr in rand_idx:\n",
    "            ihi, iwi = np.unravel_index(rr, (height - 2 * offset, width - 2 * offset))\n",
    "            hi = (ihi+offset)*(input.shape[2]/height)-Apad\n",
    "            wi = (iwi + offset)*(input.shape[3]/width)-Apad\n",
    "            #hi = Astride * (ihi + offset) - Apad\n",
    "            #wi = Astride * (iwi + offset) - Apad\n",
    "\n",
    "            #assert (hi >= 0)\n",
    "            #assert (wi >= 0)\n",
    "            #assert (hi <= img.shape[0] - Arf)\n",
    "            #assert (wi <= img.shape[1] - Arf)\n",
    "            loc_set.append([category, ii, hi,wi,hi+Arf,wi+Arf])\n",
    "            feat_set.append(tmp_feats[cnt,:])\n",
    "            cnt+=1\n",
    "\n",
    "        imgs_par_cat[label]+=1\n",
    "\n",
    "\n",
    "feat_set = np.asarray(feat_set)\n",
    "loc_set = np.asarray(loc_set).T\n",
    "\n",
    "print(feat_set.shape)\n",
    "model = vMFMM(vc_num, 'k++')\n",
    "model.fit(feat_set, vMF_kappa, max_it=150)\n",
    "with open(dict_dir+'dictionary_{}_{}.pickle'.format(layer,vc_num), 'wb') as fh:\n",
    "    pickle.dump(model.mu, fh)\n",
    "\n",
    "\n",
    "num = 50\n",
    "SORTED_IDX = []\n",
    "SORTED_LOC = []\n",
    "for vc_i in range(vc_num):\n",
    "    sort_idx = np.argsort(-model.p[:, vc_i])[0:num]\n",
    "    SORTED_IDX.append(sort_idx)\n",
    "    tmp=[]\n",
    "    for idx in range(num):\n",
    "        iloc = loc_set[:, sort_idx[idx]]\n",
    "        tmp.append(iloc)\n",
    "    SORTED_LOC.append(tmp)\n",
    "\n",
    "with open(dict_dir + 'dictionary_{}_{}_p.pickle'.format(layer,vc_num), 'wb') as fh:\n",
    "    pickle.dump(model.p, fh)\n",
    "p = model.p\n",
    "\n",
    "print('save top {0} images for each cluster'.format(num))\n",
    "example = [None for vc_i in range(vc_num)]\n",
    "out_dir = os.path.join(dict_dir, f'cluster_images_{layer}_{vc_num}')\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for vc_i in range(vc_num):\n",
    "    patch_set = np.zeros(((Arf**2)*3, num)).astype('uint8')\n",
    "    sort_idx = SORTED_IDX[vc_i]#np.argsort(-p[:,vc_i])[0:num]\n",
    "    opath = os.path.join(out_dir, str(vc_i))\n",
    "    if not os.path.exists(opath):\n",
    "        os.makedirs(opath)\n",
    "    locs=[]\n",
    "    for idx in range(num):\n",
    "        iloc = loc_set[:,sort_idx[idx]]\n",
    "        category = iloc[0]\n",
    "        loc = iloc[1:6].astype(int)\n",
    "        if not loc[0] in locs:\n",
    "            locs.append(loc[0])\n",
    "#             pdb.set_trace()\n",
    "            \n",
    "            archive_img_path = archive.open(imgs[int(loc[0])])\n",
    "            img = np.array(Image.open(archive_img_path))\n",
    "            img = myresize(img, 224, 'short')\n",
    "            patch = img[loc[1]:loc[3], loc[2]:loc[4], :]\n",
    "            #patch_set[:,idx] = patch.flatten()\n",
    "            if patch.size:\n",
    "                out_path = os.path.join(opath, f'{str(idx)}.JPEG')\n",
    "                cv2.imwrite(out_path, patch)\n",
    "    #example[vc_i] = np.copy(patch_set)\n",
    "    if vc_i%10 == 0:\n",
    "        print(vc_i)\n",
    "\n",
    "# print summary for each vc\n",
    "#if layer=='pool4' or layer =='last': # somehow the patches seem too big for p5\n",
    "for c in range(vc_num):\n",
    "    iidir = os.path.join(out_dir, str(c))\n",
    "    files = glob.glob(iidir+'*.JPEG')\n",
    "    width = 100\n",
    "    height = 100\n",
    "    canvas = np.zeros((0,4*width,3))\n",
    "    cnt = 0\n",
    "    for jj in range(4):\n",
    "        row = np.zeros((height,0,3))\n",
    "        ii=0\n",
    "        tries=0\n",
    "        next=False\n",
    "        for ii in range(4):\n",
    "            if (jj*4+ii)< len(files):\n",
    "                img_file = files[jj*4+ii]\n",
    "#                 archive_img_path = archive.open(img_file)\n",
    "#                 img = np.array(Image.open(archive_img_path))\n",
    "                \n",
    "                if os.path.exists(img_file):\n",
    "                    img = cv2.imread(img_file)\n",
    "                img = cv2.resize(img, (width,height))\n",
    "            else:\n",
    "                img = np.zeros((height, width, 3))\n",
    "            row = np.concatenate((row, img), axis=1)\n",
    "        canvas = np.concatenate((canvas,row),axis=0)\n",
    "    cv2.imwrite(os.path.join(out_dir, f'{str(c)}.JPEG'),canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictfile = os.path.join(dict_dir, f'dictionary_{layer}_{vc_num}.pickle')\n",
    "with open(dictfile, 'rb') as fh:\n",
    "    centers = pickle.load(fh)\n",
    "bool_pytroch = True\n",
    "bool_plot_view_p3d=False\n",
    "\n",
    "mixdir = init_path + 'mix_model_vmf_{}_EM_all/'.format(dataset)\n",
    "if not os.path.exists(mixdir):\n",
    "    os.makedirs(mixdir)\n",
    "occ_level='ZERO'\n",
    "occ_type=''\n",
    "spectral_split_thresh=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading /project/6052161/mattlk/workplace/CompNet-Medical/CompositionalNets/models/init_vgg/dictionary_vgg/dictionary_pool5_512.pickle\n"
     ]
    }
   ],
   "source": [
    "from CompositionalNets.Initialization_Code.Learn_mix_model_vMF_view import learn_mix_model_vMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn_mix_model_vMF"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
