{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_images 5000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2b98e971d390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gzip\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import pydicom\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pdb\n",
    "\n",
    "from CompositionalNets.Code.config import vc_num\n",
    "from CompositionalNets.Initialization_Code.comptSimMat import compute_similarity_matrix\n",
    "from CompositionalNets.Initialization_Code.vMF_clustering import learn_vmf_clusters, save_cluster_images\n",
    "from CompositionalNets.Initialization_Code.Learn_mix_model_vMF_view import learn_mix_model_vMF\n",
    "from CompositionalNets.Initialization_Code.config_initialization import nn_type\n",
    "\n",
    "from src.config import Directories\n",
    "from src.models import UNet\n",
    "from src.dataset import ClassificationDataset, apply_ct_abdomen_filter, get_chaos_volumes, TripleDataset\n",
    "from src.utils import synthetic_occlusion\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattlk/.virtualenvs/CompNet/lib/python3.6/site-packages/ipykernel_launcher.py:25: DeprecationWarning: use the name attribute\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.8 s, sys: 2.91 s, total: 14.7 s\n",
      "Wall time: 16.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tight_crop = True\n",
    "\n",
    "# TODO make this faster by reading slices from h5py instead of directly\n",
    "# slicing from the volume\n",
    "# vol_dir = os.path.join(Directories.CHAOS_REGISTRATIONS, 'affine', 'out')\n",
    "vol_dir = os.path.join(Directories.CHAOS_REGISTRATIONS, 'affine')\n",
    "regex = re.compile('.*\\.nii\\.gz')\n",
    "names = [f for f in sorted(os.listdir(vol_dir)) if regex.match(f)]\n",
    "slices = []\n",
    "slice_range = (-16, 16)\n",
    "volume_filepaths = get_chaos_volumes(Directories.CHAOS)\n",
    "\n",
    "# use_masks = 'out' not in vol_dir\n",
    "masks = []\n",
    "use_masks = False\n",
    "# all_masks = []\n",
    "\n",
    "for i, name in enumerate(names):\n",
    "    \n",
    "    pairs = volume_filepaths[i]\n",
    "    fp = os.path.join(vol_dir, name)\n",
    "    \n",
    "    with gzip.open(fp, 'rb') as f:\n",
    "\n",
    "        niftii_object = nib.load(f.filename)\n",
    "        volume = np.array(niftii_object.dataobj, dtype=np.int16)\n",
    "        \n",
    "        num_slices = volume.shape[2]\n",
    "        amount = np.zeros(num_slices)\n",
    "        \n",
    "        assert num_slices == len(pairs)\n",
    "        \n",
    "        for j in range(num_slices):\n",
    "\n",
    "            s = volume[...,j]\n",
    "            background = s.min()\n",
    "            amount[j] = np.sum(s==background)\n",
    "        \n",
    "        idx = np.argmin(amount)\n",
    "#         idx = 60\n",
    "        \n",
    "        if use_masks:\n",
    "            for k in range(idx+slice_range[0], idx+slice_range[1]):\n",
    "                _, mask_fp = pairs[k]\n",
    "                mask = np.array(Image.open(mask_fp), dtype=np.uint8)\n",
    "                masks.append(mask)\n",
    "\n",
    "        curr_slices = volume[..., idx+slice_range[0]:idx+slice_range[1]]\n",
    "        \n",
    "        curr_slices = apply_ct_abdomen_filter(curr_slices)\n",
    "\n",
    "        curr_min, curr_max = curr_slices.min(), curr_slices.max()\n",
    "        curr_slices = (curr_slices - curr_min) / (curr_max - curr_min + 1e-12)\n",
    "\n",
    "        curr_slices = np.transpose(curr_slices, (2, 0, 1))\n",
    "        curr_slices = np.stack((curr_slices, curr_slices, curr_slices), axis=len(curr_slices.shape))\n",
    "        curr_slices = np.rot90(curr_slices, k=1, axes=(1,2))\n",
    "        curr_slices = curr_slices.astype(np.float32)\n",
    "        \n",
    "        # Crop around liver\n",
    "        if tight_crop:\n",
    "            curr_slices = [img[np.ix_((img[...,0]>0).any(1), (img[...,0]>0).any(0))] for img in curr_slices]\n",
    "        \n",
    "        slices.extend(curr_slices)\n",
    "        \n",
    "slices = np.asarray(slices)\n",
    "if use_masks:\n",
    "    train_slices, test_slices, train_masks, test_masks = train_test_split(slices, masks, test_size=0.2, random_state=42)\n",
    "    dataset = TripleDataset(train_slices, train_masks, [0] * len(train_slices), X_transform=transforms.ToTensor())\n",
    "else:\n",
    "    train_slices, test_slices = train_test_split(slices, test_size=0.2, random_state=42)\n",
    "    dataset = ClassificationDataset(train_slices, [0] * len(train_slices), input_transform=transforms.ToTensor())\n",
    "    \n",
    "data_loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "synthetic_images = synthetic_occlusion(test_slices[:20], textured=False, color=0.78431374)\n",
    "for i in range(8):\n",
    "    try:\n",
    "        im = Image.open(os.path.join(Directories.DATA, 'tumors', f'tumor_slice{i}.jpg'))\n",
    "        im = (np.array(im).astype(np.float32) / 255)\n",
    "        synthetic_images[i] = im\n",
    "    except Exception as e:\n",
    "        print(f'{e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kappa in range(60, 61, 10):\n",
    "    experiment = 32\n",
    "    kappas = [kappa]\n",
    "    vmf, loc_set =  learn_vmf_clusters(data_loader, img_per_cat=len(dataset), verbose=True,\n",
    "                                       max_it=1000, tol=5e-12,\n",
    "                                       u_out_name=f'chaos_pool5_{vc_num}_u_test_{experiment}.pickle',\n",
    "                                       p_out_name=f'chaos_pool5_{vc_num}_p_test_{experiment}.pickle',\n",
    "                                       kappas=kappas)\n",
    "    save_cluster_images(vmf, loc_set, in_images=train_slices*255,\n",
    "                    num_images=16, out_dir_name=f'test_{experiment}_{kappa}',\n",
    "                    max_num_clusters=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Sampling DNN features from dataset', la…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f57de198d44a44cc9a2bd6bcaeb67b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Selecting best Kappa', max=1.0, style=ProgressStyle(descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Kappa: 5. Lowest Similarity: 0.9999976003184385\n"
     ]
    }
   ],
   "source": [
    "experiment = 41\n",
    "vmf, loc_set =  learn_vmf_clusters(data_loader, img_per_cat=len(dataset), verbose=True,\n",
    "                                   max_it=1000, tol=5e-12,\n",
    "                                   u_out_name=f'chaos_pool5_{vc_num}_u_test_{experiment}.pickle',\n",
    "                                   p_out_name=f'chaos_pool5_{vc_num}_p_test_{experiment}.pickle',\n",
    "                                   kappas=range(5, 6, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving top 16 images for each cluster\n"
     ]
    }
   ],
   "source": [
    "save_cluster_images(vmf, loc_set, in_images=train_slices*255,\n",
    "                    num_images=16, out_dir_name=f'test_{experiment}',\n",
    "                    max_num_clusters=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determine best threshold for binarization - 0 ...\n",
      "Start compute sim matrix ... magicThresh 0.1\n",
      "iter 1/11 1/11\n",
      "comptSimMat iter time: 91.19228333632151\n",
      "iter 1/11 2/11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m/project/6052161/mattlk/workplace/CompNet-Medical/CompositionalNets/Initialization_Code/comptSimMat.py\u001b[0m in \u001b[0;36mcompute_similarity_matrix\u001b[0;34m(data_loader, category, save_name, sim_dir_name, u_out_name, N_sub, num_layer_features)\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mlayer_feature_b_ss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_feature_b\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_feature_b_ss2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlfb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlfb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayer_feature_b_ss1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m                 \u001b[0mpara_rst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparal_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvc_dis_paral_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mmat_dis1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpara_rst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/CompNet/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    538\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/cvmfs/soft.computecanada.ca/easybuild/software/2017/Core/python/3.6.3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mat1, mat2 = compute_similarity_matrix(data_loader, 0, f'test_{experiment}',\n",
    "                                       sim_dir_name=f'similarity_{nn_type}_pool5_chaos_{experiment}',\n",
    "                                       u_out_name=f'chaos_pool5_{vc_num}_u_test_{experiment}.pickle',\n",
    "                                       N_sub=min(200, len(dataset)//10), num_layer_features=min(100, len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/project/6052161/mattlk/workplace/CompNet-Medical/CompositionalNets/models/init_unet/similarity_unet_pool5_chaos_41/test_41'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2f1525397bc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mdict_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'chaos_pool5_{vc_num}_u_test_{experiment}.pickle'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mmixdir_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'mix_model_vmf_chaos_EM_all_test_{experiment}/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                     im_channels=3)\n\u001b[0m",
      "\u001b[0;32m/project/6052161/mattlk/workplace/CompNet-Medical/CompositionalNets/Initialization_Code/Learn_mix_model_vMF_view.py\u001b[0m in \u001b[0;36mlearn_mix_model_vMF\u001b[0;34m(data_loader, category, sim_matrix_name, sim_dir_name, dict_filename, mixdir_name, num_layers, num_clusters_per_layer, frac_data, occ_level, occ_type, spectral_split_thresh, im_channels)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Spectral clustering based on the similarity matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0msim_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_dir_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_matrix_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_fname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mmat_dis1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/project/6052161/mattlk/workplace/CompNet-Medical/CompositionalNets/models/init_unet/similarity_unet_pool5_chaos_41/test_41'"
     ]
    }
   ],
   "source": [
    "learn_mix_model_vMF(data_loader, 0, sim_matrix_name=f'test_{experiment}',\n",
    "                    num_layers=1, num_clusters_per_layer=1,\n",
    "                    sim_dir_name=f'similarity_{nn_type}_pool5_chaos_{experiment}',\n",
    "                    dict_filename=f'chaos_pool5_{vc_num}_u_test_{experiment}.pickle',\n",
    "                    mixdir_name=f'mix_model_vmf_chaos_EM_all_test_{experiment}/',\n",
    "                    im_channels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localize Occluders on Synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from CompositionalNets.Code.config import categories, device_ids, categories_train, mix_model_path, dict_dir, layer, vMF_kappa, compnet_type, num_mixtures\n",
    "from CompositionalNets.Code.config import config as cfg\n",
    "from CompositionalNets.Code.model import Net\n",
    "from CompositionalNets.Code.helpers import getVmfKernels, getCompositionModel\n",
    "from CompositionalNets.Code.eval_occlusion_localization import visualize_response_map, eval_occ_detection\n",
    "from CompositionalNets.Initialization_Code.config_initialization import extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = 'unet'\n",
    "num_clusters = 16\n",
    "experiment = 40\n",
    "occ_likely = [0.6 for _ in range(len(categories_train))]\n",
    "\n",
    "dict_dir = os.path.join(Directories.COMPOSITIONAL_NETS,\n",
    "                        f'models/init_{encoder}/dictionary_{encoder}/chaos_pool5_{num_clusters}_u_test_{experiment}.pickle')\n",
    "weights = getVmfKernels(dict_dir, device_ids)\n",
    "mix_model_path = os.path.join(Directories.COMPOSITIONAL_NETS, \n",
    "                             f'models/init_{encoder}/mix_model_vmf_chaos_EM_all_test_{experiment}/')\n",
    "mix_models = getCompositionModel(device_ids, mix_model_path, layer,\n",
    "                                 [0],\n",
    "                                 compnet_type=compnet_type,\n",
    "                                 num_mixtures=num_mixtures)\n",
    "\n",
    "model = Net(extractor, weights, vMF_kappa, occ_likely, mix_models,\n",
    "            bool_mixture_bg=True,\n",
    "            compnet_type=compnet_type, num_mixtures=num_mixtures, \n",
    "            vc_thresholds=cfg.MODEL.VC_THRESHOLD, occlusion_threshold=19)\n",
    "if device_ids:\n",
    "    model.to(device_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "i = 0\n",
    "out_dir = f'{Directories.COMPOSITIONAL_NETS}/models/init_{encoder}/occlusion_maps/test_{experiment}/'\n",
    "for image in synthetic_images[:]:\n",
    "    \n",
    "    if tight_crop:\n",
    "        image = image[np.ix_((image[...,0]>0).any(1), (image[...,0]>0).any(0))]\n",
    "    image = cc(image)\n",
    "\n",
    "    if device_ids:\n",
    "        image = image.cuda(device_ids[0])\n",
    "\n",
    "    image = image.unsqueeze(0)\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            score, occ_maps, part_scores = model.get_occlusion(image, 0)\n",
    "\n",
    "        occ_map = occ_maps[0].detach().cpu().numpy()\n",
    "        occ_map = cv2.medianBlur(occ_map.astype(np.float32), 3)\n",
    "        occ_img = visualize_response_map(occ_map, tit='', cbarmax=0)\n",
    "\n",
    "        img_orig = (image[0].permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n",
    "        faco = img_orig.shape[0] / occ_img.shape[0]\n",
    "\n",
    "        occ_img_s = cv2.resize(occ_img, (int(occ_img.shape[1] * faco), img_orig.shape[0]))[:,:img_orig.shape[1],:]\n",
    "\n",
    "        canvas = np.concatenate((img_orig, occ_img_s), axis=1)\n",
    "        plt.figure(figsize=(7, 7))\n",
    "        plt.imshow(canvas)\n",
    "        plt.axis('off')\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localize Real Tumors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tight_crop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(Directories.LITS, 'media', 'nas', '01_Datasets', 'CT', 'LITS')\n",
    "data_train_dir = os.path.join(data_dir, 'Training Set')\n",
    "data_test_dir = os.path.join(data_dir, 'Testing Set')\n",
    "\n",
    "volume_filepaths = [os.path.join(data_train_dir, name) for name in sorted(os.listdir(data_train_dir)) \n",
    "                    if 'volume' in name]\n",
    "segmentation_filepaths = [os.path.join(data_train_dir, name) for name in sorted(os.listdir(data_train_dir)) \n",
    "                          if 'segmentation' in name]\n",
    "\n",
    "pairs = [(vol, gt) for vol, gt in zip(volume_filepaths, segmentation_filepaths)]\n",
    "\n",
    "cc = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "out_dir = f'{Directories.COMPOSITIONAL_NETS}/models/init_{nn_type}/occlusion_maps/test_lits_{experiment}/'\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "\n",
    "for pair_num, (volume_fp, segmentation_fp) in enumerate(pairs[3:4]):\n",
    "    i = 0\n",
    "    \n",
    "    volume = nib.load(volume_fp)\n",
    "    segmentation = nib.load(segmentation_fp)\n",
    "    \n",
    "    volume_data = volume.get_fdata()\n",
    "    segmentation_data = segmentation.get_fdata()\n",
    "    vol_min = volume_data.min()\n",
    "    vol_max = volume_data.max()\n",
    "    \n",
    "    _, _, num_slices = volume_data.shape\n",
    "    \n",
    "    test_images = []\n",
    "    for j in range(num_slices):\n",
    "        image = volume_data[...,j]\n",
    "        target = segmentation_data[...,j]\n",
    "\n",
    "        image = np.array(image)\n",
    "        target = np.array(target)\n",
    "        \n",
    "        image = np.rot90(image, k=1)\n",
    "        target = np.rot90(target, k=1)\n",
    "\n",
    "        liver = target.copy()\n",
    "        liver[target == 2] = 1\n",
    "        \n",
    "        tumor = target.copy()\n",
    "        tumor[target == 1] = 0\n",
    "        tumor[target == 2] = 1\n",
    "\n",
    "        if np.sum(liver) > 10000:\n",
    "            \n",
    "            image = image * liver\n",
    "            \n",
    "            image = (image - image.min()) / (image.max() - image.min() + 1e-12)\n",
    "            image = image.astype(np.float32)\n",
    "            image = np.stack((image, image, image), axis=2)\n",
    "            \n",
    "            if tight_crop:\n",
    "                image = image[np.ix_((image[...,0]>0).any(1), (image[...,0]>0).any(0))]\n",
    "                \n",
    "            image = cc(image)\n",
    "            if device_ids:\n",
    "                image = image.cuda(device_ids[0])\n",
    "            image = image.unsqueeze(0)\n",
    "\n",
    "            score, occ_maps, part_scores = model.get_occlusion(image, 0)\n",
    "            occ_map = occ_maps[0].detach().cpu().numpy()\n",
    "            occ_map = cv2.medianBlur(occ_map.astype(np.float32), 3)\n",
    "            occ_img = visualize_response_map(occ_map, tit='', cbarmax=0)\n",
    "\n",
    "            img_orig = (image[0].permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n",
    "            faco = img_orig.shape[0] / occ_img.shape[0]\n",
    "            \n",
    "            if tight_crop:\n",
    "                img_orig = img_orig[np.ix_((img_orig[...,0]>0).any(1), (img_orig[...,0]>0).any(0))]\n",
    "            \n",
    "            occ_img_s = cv2.resize(occ_img, (int(occ_img.shape[1] * faco), img_orig.shape[0]))[:,:img_orig.shape[1],:]\n",
    "            canvas = np.concatenate((img_orig, occ_img_s, np.stack((target,target,target),axis=2)*255/2), axis=1)\n",
    "            cv2.imwrite(os.path.join(out_dir, f'{pair_num:02d}_{i:02d}.jpg'), canvas)\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor = np.stack((tumor, tumor, tumor), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = eval_occ_detection(occ_img_s, tumor)\n",
    "best_score = 0\n",
    "best_scores = []\n",
    "for scores in out:\n",
    "    acc_score = scores[-1]\n",
    "    if acc_score > best_score:\n",
    "        best_score = acc_score\n",
    "        best_scores = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ac8dc88c240>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(occ_img_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 512, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ba5db328630>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = (occ_img_s > 100) * occ_img_s\n",
    "result[result != 0] = 255\n",
    "result = result[...,2]\n",
    "plt.imshow(result, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for image, target in zip(image_dataset, target_dataset):\n",
    "        \n",
    "        image = apply_ct_abdomen_filter(image)\n",
    "    \n",
    "        image = np.stack((image, image, image), axis=2)\n",
    "\n",
    "        # Normalize between 0 and 1\n",
    "        image = (image - image_dataset.min()) / (image_dataset.max() - image_dataset.min() + 1e-12)\n",
    "        image = image.astype(np.float32)\n",
    "        \n",
    "        if tight_crop:\n",
    "            image = image[np.ix_((image[...,0]>0).any(1), (image[...,0]>0).any(0))]\n",
    "        \n",
    "        image = cc(image)\n",
    "        \n",
    "        if device_ids:\n",
    "            image = image.cuda(device_ids[0])\n",
    "        image = image.unsqueeze(0)\n",
    "        \n",
    "        image = image.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "        \n",
    "        image = cc(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.cuda()\n",
    "\n",
    "        score, occ_maps, part_scores = model.get_occlusion(image, 0)\n",
    "        occ_map = occ_maps[0].detach().cpu().numpy()\n",
    "        occ_map = cv2.medianBlur(occ_map.astype(np.float32), 3)\n",
    "        occ_img = visualize_response_map(occ_map, tit='', cbarmax=0)\n",
    "        \n",
    "        img_orig = (image[0].permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n",
    "        faco = img_orig.shape[0] / occ_img.shape[0]\n",
    "        \n",
    "        occ_img_s = cv2.resize(occ_img, (int(occ_img.shape[1] * faco), img_orig.shape[0]))\n",
    "        \n",
    "        canvas = np.concatenate((img_orig, occ_img_s), axis=1)\n",
    "#         plt.figure(figsize=(8, 8))\n",
    "#         plt.imshow(canvas)\n",
    "#         plt.axis('off')\n",
    "#         fp = f'{Directories.COMPOSITIONAL_NETS}/results/lits/train_without_occluder/test_{i}.png'\n",
    "#         cv2.imwrite(fp, canvas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tumor_dir = os.path.join(Directories.DATA, 'tumors')\n",
    "tumors = [cv2.imread(os.path.join(tumor_dir, name)) for name in os.listdir(tumor_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hdf5_fp = os.path.join(Directories.LITS, 'train.hdf5')\n",
    "with h5py.File(train_hdf5_fp, 'r') as hf:\n",
    "    image_dataset = hf['images'][40:60]\n",
    "    target_dataset = hf['masks'][40:60]\n",
    "\n",
    "tumors = []\n",
    "for im in image_dataset:\n",
    "    im = apply_ct_abdomen_filter(im)\n",
    "    \n",
    "    im = np.stack((im, im, im), axis=2)\n",
    "\n",
    "    # Normalize between 0 and 1\n",
    "    im = (im - im.min()) / (im.max() - im.min())\n",
    "    im = im.astype(np.float32)\n",
    "    tumors.append(im)\n",
    "\n",
    "unet_filename = 'unet_liver_2020-08-13_15:52:08.pth'\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if is_cuda_available else \"cpu\")\n",
    "\n",
    "path_to_unet = os.path.join(Directories.CHECKPOINTS, unet_filename)\n",
    "unet = UNet(pretrained=True)\n",
    "unet.load_state_dict(torch.load(path_to_unet)['model_state_dict'])\n",
    "if is_cuda_available: unet.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "i = 0\n",
    "with torch.no_grad():\n",
    "    for image in tumors[:10]:\n",
    "        \n",
    "        image = cc(image)\n",
    "        \n",
    "        if device_ids:\n",
    "            image = image.cuda(device_ids[0])\n",
    "        image = image.unsqueeze(0)\n",
    "        \n",
    "        out = unet(image)\n",
    "        scores = F.softmax(out, dim=1)\n",
    "\n",
    "        segmentations = torch.round(F.threshold(scores[:, 1, :, :], 0.9, 0))\n",
    "        processed_images = segmentations.unsqueeze(1) * image\n",
    "        \n",
    "        seg_liver = processed_images.squeeze()\n",
    "        image = seg_liver.unsqueeze(0)\n",
    "        \n",
    "        image = image.squeeze(0).permute(1,2,0).cpu().numpy()\n",
    "        image = image[np.ix_((image[...,0]>0).any(1), (image[...,0]>0).any(0))]\n",
    "        image = cc(image)\n",
    "        image = image.unsqueeze(0)\n",
    "        image = image.cuda()\n",
    "\n",
    "        score, occ_maps, part_scores = model.get_occlusion(image, 0)\n",
    "        occ_map = occ_maps[0].detach().cpu().numpy()\n",
    "        occ_map = cv2.medianBlur(occ_map.astype(np.float32), 3)\n",
    "        occ_img = visualize_response_map(occ_map, tit='', cbarmax=0)\n",
    "        \n",
    "        img_orig = (image[0].permute(1,2,0).cpu().numpy()*255).astype(np.uint8)\n",
    "        faco = img_orig.shape[0] / occ_img.shape[0]\n",
    "        \n",
    "        occ_img_s = cv2.resize(occ_img, (int(occ_img.shape[1] * faco), img_orig.shape[0]))\n",
    "        \n",
    "        canvas = np.concatenate((img_orig, occ_img_s), axis=1)\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(canvas)\n",
    "        plt.axis('off')\n",
    "#         fp = f'{Directories.COMPOSITIONAL_NETS}/results/lits/train_without_occluder/test_{i}.png'\n",
    "#         cv2.imwrite(fp, canvas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
