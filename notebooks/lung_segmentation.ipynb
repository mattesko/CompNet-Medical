{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pdb\n",
    "import random\n",
    "\n",
    "from comet_ml import Experiment\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import kornia.augmentation as K\n",
    "\n",
    "from src.dataset import Resize, Hdf5SegmentationDataset\n",
    "from src.models import UNet\n",
    "from src.metrics import dice_loss, dice_score\n",
    "from src.utils import create_canvas\n",
    "from src.train import train_one_epoch, validate\n",
    "from src.config import directories\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 # 0: off, 2: on for all modules\n",
    "# os.chdir('CompositionalNets/')\n",
    "# sys.path.append('/project/6052161/mattlk/workplace/CompNet')\n",
    "\n",
    "data_dir = directories['pulmonary_cxr_abnormalities']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16.4 s, sys: 5.07 s, total: 21.5 s\n",
      "Wall time: 26.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "        \"lr\": 0.0001,\n",
    "        \"batch_size\": 8,\n",
    "        \"split_train_val\": 0.8,\n",
    "        \"epochs\": 50,\n",
    "        \"use_dice_loss\": False,\n",
    "        \"cache\": True,\n",
    "        \"random_seed\": 42,\n",
    "        \"shuffle_data\": True,\n",
    "        \"scheduler\": \"StepLR\",\n",
    "        \"step_size\": 15,\n",
    "        \"gamma\": 0.75,\n",
    "        \"threshold\": 0.5,\n",
    "        \"pretrained\": True,\n",
    "#         \"change_loss_weights_at\": 50\n",
    "    }\n",
    "\n",
    "cache_input_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Resize((256, 256)),\n",
    "])\n",
    "\n",
    "cache_target_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    Resize((256, 256)),\n",
    "#     transforms.Lambda(lambda x: x.squeeze()),\n",
    "#     transforms.Lambda(lambda x: x*255),\n",
    "#     transforms.Lambda(lambda x: x.long()),\n",
    "])\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     Resize((256, 256)),\n",
    "    K.RandomAffine(0, shear=(-5, 5)),\n",
    "    K.RandomHorizontalFlip(),\n",
    "    K.CenterCrop(224),\n",
    "    transforms.Lambda(lambda x: x.squeeze()),\n",
    "])\n",
    "target_transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     Resize((256, 256)),\n",
    "    K.RandomAffine(0, shear=(-5, 5)),\n",
    "    K.RandomHorizontalFlip(),\n",
    "    K.CenterCrop(224),\n",
    "    transforms.Lambda(lambda x: x.squeeze()),\n",
    "    transforms.Lambda(lambda x: x*255),\n",
    "    transforms.Lambda(lambda x: x.long()),\n",
    "])\n",
    "val_target_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.squeeze()),\n",
    "    transforms.Lambda(lambda x: x*255),\n",
    "    transforms.Lambda(lambda x: x.long()),\n",
    "])\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if is_cuda_available else \"cpu\")\n",
    "input_dtype = torch.double\n",
    "target_dtype = torch.long\n",
    "if is_cuda_available: torch.cuda.empty_cache()\n",
    "\n",
    "hf_fp = os.path.join(data_dir, 'train.hdf5')\n",
    "\n",
    "train_dataset = Hdf5SegmentationDataset(hf_fp, 'shenzhen/healthy/cxr', 'shenzhen/healthy/masks',\n",
    "                                        input_transform=input_transform, target_transform=target_transform,\n",
    "                                        cache_input_transform=cache_input_transform,\n",
    "                                        cache_target_transform=cache_target_transform,\n",
    "                                        cache=True, distribution_name='shenzhen/healthy/target_distribution',\n",
    "                                        target_count_name='shenzhen/healthy/target_count')\n",
    "\n",
    "hf_fp = os.path.join(data_dir, 'test.hdf5')\n",
    "val_dataset = Hdf5SegmentationDataset(hf_fp, 'shenzhen/healthy/cxr', 'shenzhen/healthy/masks',\n",
    "                                      target_transform=val_target_transform,\n",
    "                                      cache_input_transform=cache_input_transform,\n",
    "                                      cache_target_transform=cache_target_transform,\n",
    "                                      cache=True)\n",
    "\n",
    "torch.manual_seed(params['random_seed'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=params['batch_size'], \n",
    "                              pin_memory=is_cuda_available, shuffle=True,\n",
    "                              num_workers=4)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=params['batch_size'], \n",
    "                            pin_memory=is_cuda_available, shuffle=True,\n",
    "                            num_workers=4)\n",
    "\n",
    "bias = train_dataset.target_count[1]/train_dataset.target_count[0]\n",
    "unet = UNet(dice=params['use_dice_loss'], pretrained=params['pretrained'])\n",
    "\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=params['lr'])\n",
    "if params['scheduler'] == 'StepLR': \n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, \n",
    "                                          step_size=params['step_size'], gamma=params['gamma'])\n",
    "elif params['scheduler'] == 'ReduceLROnPlateau':\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# cross-entropy loss: weighting of negative vs positive pixels\n",
    "loss_weight = torch.DoubleTensor(1 - train_dataset.target_distribution)\n",
    "# loss_weight = torch.DoubleTensor([0.01, 0.99])\n",
    "params['loss_weights'] = loss_weight.numpy()\n",
    "# weight_change = torch.from_numpy(train_dataset.target_distribution - loss_weight.numpy()) / (params[\"change_loss_weights_at\"] - params['epochs'])\n",
    "\n",
    "if is_cuda_available: \n",
    "    loss_weight = loss_weight.to(device)\n",
    "#     weight_change = weight_change.to(device)\n",
    "    unet = unet.to(device, dtype=input_dtype)\n",
    "    \n",
    "criterion = dice_loss if params['use_dice_loss'] else CrossEntropyLoss(weight=loss_weight,\n",
    "                                                                       reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: old comet version (3.1.14) detected. current: 3.1.15 please update your comet lib with command: `pip install --no-cache-dir --upgrade comet_ml`\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/matthew42/lung-segmentation/5e4b5944b5d643799a84e5ff22af56c5\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:\t223\n",
      "Number of validation images:\t56\n",
      "[Epoch 001 Training]\tCross-Entropy Loss:\t1.8398\n",
      "[Epoch 001 Validation]\tAverage F1 Score:\t0.0000\tAverage Jaccard/IoU:\t0.0000\n",
      "\n",
      "[Epoch 002 Training]\tCross-Entropy Loss:\t1.8328\n",
      "[Epoch 002 Validation]\tAverage F1 Score:\t0.0000\tAverage Jaccard/IoU:\t0.0000\n",
      "\n",
      "[Epoch 003 Training]\tCross-Entropy Loss:\t1.8223\n",
      "[Epoch 003 Validation]\tAverage F1 Score:\t0.0000\tAverage Jaccard/IoU:\t0.0000\n",
      "\n",
      "[Epoch 004 Training]\tCross-Entropy Loss:\t1.8053\n",
      "[Epoch 004 Validation]\tAverage F1 Score:\t0.0115\tAverage Jaccard/IoU:\t0.0058\n",
      "\n",
      "[Epoch 005 Training]\tCross-Entropy Loss:\t1.7891\n",
      "[Epoch 005 Validation]\tAverage F1 Score:\t0.4659\tAverage Jaccard/IoU:\t0.2329\n",
      "\n",
      "[Epoch 006 Training]\tCross-Entropy Loss:\t1.7479\n",
      "[Epoch 006 Validation]\tAverage F1 Score:\t0.3597\tAverage Jaccard/IoU:\t0.1798\n",
      "\n",
      "[Epoch 007 Training]\tCross-Entropy Loss:\t1.6055\n",
      "[Epoch 007 Validation]\tAverage F1 Score:\t0.4341\tAverage Jaccard/IoU:\t0.2170\n",
      "\n",
      "[Epoch 008 Training]\tCross-Entropy Loss:\t1.5024\n",
      "[Epoch 008 Validation]\tAverage F1 Score:\t0.4199\tAverage Jaccard/IoU:\t0.2100\n",
      "\n",
      "[Epoch 009 Training]\tCross-Entropy Loss:\t1.4826\n",
      "[Epoch 009 Validation]\tAverage F1 Score:\t0.5034\tAverage Jaccard/IoU:\t0.2517\n",
      "\n",
      "[Epoch 010 Training]\tCross-Entropy Loss:\t1.3382\n",
      "[Epoch 010 Validation]\tAverage F1 Score:\t0.5965\tAverage Jaccard/IoU:\t0.2982\n",
      "\n",
      "[Epoch 011 Training]\tCross-Entropy Loss:\t1.2089\n",
      "[Epoch 011 Validation]\tAverage F1 Score:\t0.7018\tAverage Jaccard/IoU:\t0.3509\n",
      "\n",
      "[Epoch 012 Training]\tCross-Entropy Loss:\t1.1055\n",
      "[Epoch 012 Validation]\tAverage F1 Score:\t0.8153\tAverage Jaccard/IoU:\t0.4077\n",
      "\n",
      "[Epoch 013 Training]\tCross-Entropy Loss:\t1.0649\n",
      "[Epoch 013 Validation]\tAverage F1 Score:\t0.8354\tAverage Jaccard/IoU:\t0.4177\n",
      "\n",
      "[Epoch 014 Training]\tCross-Entropy Loss:\t1.0376\n",
      "[Epoch 014 Validation]\tAverage F1 Score:\t0.8708\tAverage Jaccard/IoU:\t0.4354\n",
      "\n",
      "[Epoch 015 Training]\tCross-Entropy Loss:\t0.9899\n",
      "[Epoch 015 Validation]\tAverage F1 Score:\t0.8800\tAverage Jaccard/IoU:\t0.4400\n",
      "\n",
      "[Epoch 016 Training]\tCross-Entropy Loss:\t0.9358\n",
      "[Epoch 016 Validation]\tAverage F1 Score:\t0.8879\tAverage Jaccard/IoU:\t0.4439\n",
      "\n",
      "[Epoch 017 Training]\tCross-Entropy Loss:\t0.9095\n",
      "[Epoch 017 Validation]\tAverage F1 Score:\t0.8848\tAverage Jaccard/IoU:\t0.4424\n",
      "\n",
      "[Epoch 018 Training]\tCross-Entropy Loss:\t0.8713\n",
      "[Epoch 018 Validation]\tAverage F1 Score:\t0.8916\tAverage Jaccard/IoU:\t0.4458\n",
      "\n",
      "[Epoch 019 Training]\tCross-Entropy Loss:\t0.8313\n",
      "[Epoch 019 Validation]\tAverage F1 Score:\t0.8925\tAverage Jaccard/IoU:\t0.4463\n",
      "\n",
      "[Epoch 020 Training]\tCross-Entropy Loss:\t0.7772\n",
      "[Epoch 020 Validation]\tAverage F1 Score:\t0.8922\tAverage Jaccard/IoU:\t0.4461\n",
      "\n",
      "[Epoch 021 Training]\tCross-Entropy Loss:\t0.7511\n",
      "[Epoch 021 Validation]\tAverage F1 Score:\t0.9002\tAverage Jaccard/IoU:\t0.4501\n",
      "\n",
      "[Epoch 022 Training]\tCross-Entropy Loss:\t0.6865\n",
      "[Epoch 022 Validation]\tAverage F1 Score:\t0.9006\tAverage Jaccard/IoU:\t0.4503\n",
      "\n",
      "[Epoch 023 Training]\tCross-Entropy Loss:\t0.6283\n",
      "[Epoch 023 Validation]\tAverage F1 Score:\t0.9079\tAverage Jaccard/IoU:\t0.4539\n",
      "\n",
      "[Epoch 024 Training]\tCross-Entropy Loss:\t0.5778\n",
      "[Epoch 024 Validation]\tAverage F1 Score:\t0.9108\tAverage Jaccard/IoU:\t0.4554\n",
      "\n",
      "[Epoch 025 Training]\tCross-Entropy Loss:\t0.4972\n",
      "[Epoch 025 Validation]\tAverage F1 Score:\t0.9160\tAverage Jaccard/IoU:\t0.4580\n",
      "\n",
      "[Epoch 026 Training]\tCross-Entropy Loss:\t0.4423\n",
      "[Epoch 026 Validation]\tAverage F1 Score:\t0.9152\tAverage Jaccard/IoU:\t0.4576\n",
      "\n",
      "[Epoch 027 Training]\tCross-Entropy Loss:\t0.4189\n",
      "[Epoch 027 Validation]\tAverage F1 Score:\t0.9083\tAverage Jaccard/IoU:\t0.4542\n",
      "\n",
      "[Epoch 028 Training]\tCross-Entropy Loss:\t0.4304\n",
      "[Epoch 028 Validation]\tAverage F1 Score:\t0.8895\tAverage Jaccard/IoU:\t0.4447\n",
      "\n",
      "[Epoch 029 Training]\tCross-Entropy Loss:\t0.3924\n",
      "[Epoch 029 Validation]\tAverage F1 Score:\t0.8985\tAverage Jaccard/IoU:\t0.4492\n",
      "\n",
      "[Epoch 030 Training]\tCross-Entropy Loss:\t0.3946\n",
      "[Epoch 030 Validation]\tAverage F1 Score:\t0.8948\tAverage Jaccard/IoU:\t0.4474\n",
      "\n",
      "[Epoch 031 Training]\tCross-Entropy Loss:\t0.3981\n",
      "[Epoch 031 Validation]\tAverage F1 Score:\t0.9005\tAverage Jaccard/IoU:\t0.4503\n",
      "\n",
      "[Epoch 032 Training]\tCross-Entropy Loss:\t0.3748\n",
      "[Epoch 032 Validation]\tAverage F1 Score:\t0.8953\tAverage Jaccard/IoU:\t0.4477\n",
      "\n",
      "[Epoch 033 Training]\tCross-Entropy Loss:\t0.3563\n",
      "[Epoch 033 Validation]\tAverage F1 Score:\t0.9032\tAverage Jaccard/IoU:\t0.4516\n",
      "\n",
      "[Epoch 034 Training]\tCross-Entropy Loss:\t0.3503\n",
      "[Epoch 034 Validation]\tAverage F1 Score:\t0.9073\tAverage Jaccard/IoU:\t0.4537\n",
      "\n",
      "[Epoch 035 Training]\tCross-Entropy Loss:\t0.3556\n",
      "[Epoch 035 Validation]\tAverage F1 Score:\t0.9153\tAverage Jaccard/IoU:\t0.4576\n",
      "\n",
      "[Epoch 036 Training]\tCross-Entropy Loss:\t0.3644\n",
      "[Epoch 036 Validation]\tAverage F1 Score:\t0.9177\tAverage Jaccard/IoU:\t0.4589\n",
      "\n",
      "[Epoch 037 Training]\tCross-Entropy Loss:\t0.3456\n",
      "[Epoch 037 Validation]\tAverage F1 Score:\t0.9162\tAverage Jaccard/IoU:\t0.4581\n",
      "\n",
      "[Epoch 038 Training]\tCross-Entropy Loss:\t0.3366\n",
      "[Epoch 038 Validation]\tAverage F1 Score:\t0.9173\tAverage Jaccard/IoU:\t0.4586\n",
      "\n",
      "[Epoch 039 Training]\tCross-Entropy Loss:\t0.3454\n",
      "[Epoch 039 Validation]\tAverage F1 Score:\t0.9207\tAverage Jaccard/IoU:\t0.4603\n",
      "\n",
      "[Epoch 040 Training]\tCross-Entropy Loss:\t0.3286\n",
      "[Epoch 040 Validation]\tAverage F1 Score:\t0.9208\tAverage Jaccard/IoU:\t0.4604\n",
      "\n",
      "[Epoch 041 Training]\tCross-Entropy Loss:\t0.3380\n",
      "[Epoch 041 Validation]\tAverage F1 Score:\t0.9186\tAverage Jaccard/IoU:\t0.4593\n",
      "\n",
      "[Epoch 042 Training]\tCross-Entropy Loss:\t0.3184\n",
      "[Epoch 042 Validation]\tAverage F1 Score:\t0.9215\tAverage Jaccard/IoU:\t0.4608\n",
      "\n",
      "[Epoch 043 Training]\tCross-Entropy Loss:\t0.3186\n",
      "[Epoch 043 Validation]\tAverage F1 Score:\t0.9218\tAverage Jaccard/IoU:\t0.4609\n",
      "\n",
      "[Epoch 044 Training]\tCross-Entropy Loss:\t0.3130\n",
      "[Epoch 044 Validation]\tAverage F1 Score:\t0.9222\tAverage Jaccard/IoU:\t0.4611\n",
      "\n",
      "[Epoch 045 Training]\tCross-Entropy Loss:\t0.3147\n",
      "[Epoch 045 Validation]\tAverage F1 Score:\t0.9231\tAverage Jaccard/IoU:\t0.4615\n",
      "\n",
      "[Epoch 046 Training]\tCross-Entropy Loss:\t0.3086\n",
      "[Epoch 046 Validation]\tAverage F1 Score:\t0.9226\tAverage Jaccard/IoU:\t0.4613\n",
      "\n",
      "[Epoch 047 Training]\tCross-Entropy Loss:\t0.3228\n",
      "[Epoch 047 Validation]\tAverage F1 Score:\t0.9231\tAverage Jaccard/IoU:\t0.4615\n",
      "\n",
      "[Epoch 048 Training]\tCross-Entropy Loss:\t0.3203\n",
      "[Epoch 048 Validation]\tAverage F1 Score:\t0.9237\tAverage Jaccard/IoU:\t0.4618\n",
      "\n",
      "[Epoch 049 Training]\tCross-Entropy Loss:\t0.3152\n",
      "[Epoch 049 Validation]\tAverage F1 Score:\t0.9237\tAverage Jaccard/IoU:\t0.4619\n",
      "\n",
      "[Epoch 050 Training]\tCross-Entropy Loss:\t0.3182\n",
      "[Epoch 050 Validation]\tAverage F1 Score:\t0.9236\tAverage Jaccard/IoU:\t0.4618\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n",
      "COMET INFO: Waiting for completion of the file uploads (may take several seconds)\n",
      "COMET INFO: Still uploading\n",
      "COMET INFO: Still uploading\n",
      "COMET INFO: Still uploading\n",
      "COMET INFO: Still uploading\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"P5seMqEJjqZ8mDA7QYSuK3yUJ\",\n",
    "                        project_name=\"lung-segmentation\", workspace=\"matthew42\")\n",
    "\n",
    "num_train, num_val = len(train_dataset), len(val_dataset)\n",
    "params['num_samples'] = num_train + num_val\n",
    "params['target_transform'] = target_transform.__str__()\n",
    "params['input_transform'] = input_transform.__str__()\n",
    "\n",
    "experiment.log_parameters(params)\n",
    "num_accumulated_steps = 128 // params['batch_size']\n",
    "\n",
    "with experiment.train():\n",
    "    \n",
    "    print(f'Number of training images:\\t{num_train}\\nNumber of validation images:\\t{num_val}')\n",
    "    \n",
    "    for epoch in range(params['epochs']):\n",
    "\n",
    "        unet, running_loss = train_one_epoch(unet, train_dataloader, optimizer,\n",
    "                                             criterion, num_accumulated_steps=num_accumulated_steps,\n",
    "                                             device=device, input_dtype=input_dtype, target_dtype=target_dtype,\n",
    "                                             **params)\n",
    "\n",
    "        if params['use_dice_loss']:\n",
    "            print(f'[Epoch {epoch+1:03d} Training]\\tDice Loss:\\t\\t{running_loss:.4f}')\n",
    "        else:\n",
    "            print(f'[Epoch {epoch+1:03d} Training]\\tCross-Entropy Loss:\\t{running_loss:.4f}')\n",
    "            experiment.log_metric(\"Running Loss\", running_loss, epoch=epoch, step=epoch, include_context=False)\n",
    "        \n",
    "        f1_mean, jaccard_mean = validate(unet, val_dataloader, epoch, device,\n",
    "                                         batch_freq=3, epoch_freq=10,\n",
    "                                         experiment=experiment, **params)\n",
    "\n",
    "        if params['scheduler'] == 'ReduceLROnPlateau':\n",
    "            scheduler.step(f1_mean)\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        print(f'[Epoch {epoch+1:03d} Validation]\\tAverage F1 Score:\\t{f1_mean:.4f}\\tAverage Jaccard/IoU:\\t{jaccard_mean:.4f}\\n')\n",
    "\n",
    "        experiment.log_metric('Validation Average F1 Score', f1_mean,\n",
    "                              epoch=epoch, include_context=False)\n",
    "        experiment.log_metric('Validation Average Jaccard/IoU', jaccard_mean,\n",
    "                              epoch=epoch, include_context=False)\n",
    "        \n",
    "#         if epoch >= params[\"change_loss_weights_at\"]:\n",
    "#             criterion.weight = criterion.weight + weight_change\n",
    "\n",
    "date_time = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
    "filepath = os.path.join(directories['checkpoints'], f'unet_lung_{date_time}.pth')\n",
    "# torch.save(unet.state_dict(), filepath)\n",
    "torch.save({\n",
    "#     'epoch': epoch,\n",
    "    'model_state_dict': unet.state_dict(),\n",
    "#     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     'scheduler_state_dict': scheduler.state_dict(),\n",
    "    }, filepath)\n",
    "experiment.log_asset(filepath, copy_to_tmp=False)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unet.train()\n",
    "torch.set_grad_enabled(True)\n",
    "optimizer.zero_grad()\n",
    "running_loss = 0.0\n",
    "use_dice_loss = False\n",
    "\n",
    "for i, data in tqdm(enumerate(train_dataloader)):\n",
    "\n",
    "    input_images, targets = data\n",
    "\n",
    "    if device:\n",
    "        input_images = input_images.to(device, input_dtype)\n",
    "        targets = targets.to(device, target_dtype)\n",
    "    pdb.set_trace()\n",
    "    outputs = unet(input_images)\n",
    "\n",
    "    if use_dice_loss:\n",
    "        outputs = F.log_softmax(outputs, dim=1)\n",
    "        outputs = outputs[:, 1, :, :].unsqueeze(dim=1)\n",
    "        loss = criterion(outputs, targets)\n",
    "    else:\n",
    "        targets = targets.squeeze(dim=1)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    running_loss += loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_accumulated_steps = 128 // params['batch_size']\n",
    "\n",
    "unet.train()\n",
    "torch.set_grad_enabled(True)\n",
    "optimizer.zero_grad()\n",
    "running_loss = 0.0\n",
    "use_dice_loss = False\n",
    "\n",
    "for i, data in enumerate(train_dataloader):\n",
    "\n",
    "    input_images, targets = data\n",
    "\n",
    "    if device:\n",
    "        input_images = input_images.to(device, input_dtype)\n",
    "        targets = targets.to(device, target_dtype)\n",
    "    pdb.set_trace()\n",
    "    outputs = unet(input_images)\n",
    "\n",
    "    if use_dice_loss:\n",
    "        outputs = F.log_softmax(outputs, dim=1)\n",
    "        outputs = outputs[:, 1, :, :].unsqueeze(dim=1)\n",
    "        loss = criterion(outputs, targets)\n",
    "    else:\n",
    "        targets = targets.squeeze(dim=1)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "    loss /= num_accumulated_steps\n",
    "    loss.backward()\n",
    "    running_loss += loss.detach().cpu().numpy()\n",
    "    print(running_loss)\n",
    "\n",
    "    if i % num_accumulated_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss /= num_accumulated_steps\n",
    "\n",
    "if i % num_accumulated_steps != 0:\n",
    "    optimizer.step()\n",
    "    running_loss /= (num_accumulated_steps - i % num_accumulated_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, target = train_dataset[0]\n",
    "cc = transforms.Compose([\n",
    "    K.RandomAffine(0, shear=(-5, 5)),\n",
    "    K.RandomHorizontalFlip(),\n",
    "    K.CenterCrop(230),\n",
    "])\n",
    "seed = np.random.randint(2147483647)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "image = cc(image)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "target = cc(target)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(image.cpu().squeeze().permute(1, 2, 0).numpy())\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(target.cpu().squeeze().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('CompNet': conda)",
   "language": "python",
   "name": "python361064bitcompnetconda53738006ed9d4ff8b58cccf30ed5af95"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}