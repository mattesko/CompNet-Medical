{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "import warnings\n",
    "import io\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from comet_ml import Experiment\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import jaccard_score, f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from dataset import Chaos2DSegmentationDataset, NormalizeInstance, get_image_pair_filepaths\n",
    "from models import UNet\n",
    "from metrics import dice_loss\n",
    "from utils import create_canvas\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2 # 0: off, 2: on for all modules\n",
    "# os.chdir('CompositionalNets/')\n",
    "# sys.path.append('/project/6052161/mattlk/workplace/CompNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the below directory depending on where the CHAOS dataset is stored\n",
    "data_dir = os.path.join('CompositionalNets', 'data', 'chaos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/matthew42/chaos-liver-segmentation/aefd7de71ce94bff867ff2f3ca988a49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(api_key=\"P5seMqEJjqZ8mDA7QYSuK3yUJ\",\n",
    "                        project_name=\"chaos-liver-segmentation\",\n",
    "                        workspace=\"matthew42\", auto_metric_logging=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train U-Net on CHAOS for Liver Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:\t160\n",
      "Number of validation images:\t40\n",
      "CPU times: user 1.22 s, sys: 958 ms, total: 2.18 s\n",
      "Wall time: 17.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    \"lr\": 0.0005,\n",
    "    \"batch_size\": 4,\n",
    "    \"split_train_val\": 0.8,\n",
    "    \"epochs\": 150,\n",
    "    \"use_dice_loss\": False,\n",
    "    \"random_seed\": 42,\n",
    "    \"shuffle_data\": True,\n",
    "    \"scheduler\": \"StepLR\",\n",
    "    \"step_size\": 15,\n",
    "    \"gamma\": 0.75\n",
    "}\n",
    "experiment.log_parameters(params)\n",
    "\n",
    "lr = params['lr']\n",
    "batch_size = params['batch_size']\n",
    "split_train_val = params['split_train_val']\n",
    "epochs = params['epochs']\n",
    "use_dice_loss = params['use_dice_loss']\n",
    "random_seed = params['random_seed']\n",
    "shuffle_data = params[\"shuffle_data\"]\n",
    "num_samples = 1000\n",
    "\n",
    "is_cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if is_cuda_available else \"cpu\")\n",
    "input_images_dtype = torch.double\n",
    "targets_dtype = torch.long\n",
    "cache_data = True\n",
    "\n",
    "input_transform = transforms.Compose([\n",
    "    NormalizeInstance(mean=255.0),\n",
    "    transforms.Lambda(lambda x: x.astype(np.uint8)),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "#     transforms.RandomAffine(degrees=5, shear=5),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "gt_transform = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x.astype(np.uint8)),\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "#     transforms.RandomAffine(degrees=5, shear=5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x*255),\n",
    "    transforms.Lambda(lambda x: x.long()),\n",
    "])\n",
    "\n",
    "# Load data for training and validation\n",
    "image_pair_filepaths = get_image_pair_filepaths(data_dir)[:200]\n",
    "train_filepaths, val_filepaths = train_test_split(image_pair_filepaths, train_size=split_train_val,\n",
    "                                                  random_state=random_seed, shuffle=shuffle_data)\n",
    "# train_filepaths, val_filepaths = image_pair_filepaths, image_pair_filepaths\n",
    "\n",
    "train_dataset = Chaos2DSegmentationDataset(train_filepaths, input_transform=input_transform,\n",
    "                                           gt_transform=gt_transform, cache=cache_data, device=device)\n",
    "val_dataset = Chaos2DSegmentationDataset(val_filepaths, input_transform=input_transform,\n",
    "                                         gt_transform=gt_transform, cache=cache_data, device=device)\n",
    "\n",
    "num_train, num_val = len(train_dataset), len(val_dataset)\n",
    "print(f'Number of training images:\\t{num_train}\\nNumber of validation images:\\t{num_val}')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate model, optimizer, and criterion\n",
    "torch.cuda.empty_cache()\n",
    "unet = UNet(dice=use_dice_loss)\n",
    "# unet = UNet(in_channels=1, out_channels=1, padding=0)\n",
    "if is_cuda_available: unet = unet.to(device, dtype=input_images_dtype)\n",
    "\n",
    "optimizer = optim.Adam(unet.parameters(), lr=lr)\n",
    "if params['scheduler'] == 'StepLR': \n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.75)\n",
    "elif params['scheduler'] == 'ReduceLROnPlateau':\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n",
    "\n",
    "# cross-entropy loss: weighting of negative vs positive pixels\n",
    "loss_weight = torch.DoubleTensor([0.01, 0.99])\n",
    "if is_cuda_available: loss_weight = loss_weight.to(device)\n",
    "criterion = dice_loss if use_dice_loss else CrossEntropyLoss(weight=loss_weight, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:\t160\n",
      "Number of validation images:\t40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec85b50a926e46b185ef432edc7108d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training 150 epochs', max=150, style=ProgressStyle(descriptioâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 001 Training]\tCross-Entropy Loss:\t0.3885\n",
      "[Epoch 001 Validation]\tAverage F1 Score:\t0.1462\tAverage Jaccard/IoU:\t0.0793\n",
      "\n",
      "[Epoch 002 Training]\tCross-Entropy Loss:\t0.3247\n",
      "[Epoch 002 Validation]\tAverage F1 Score:\t0.1467\tAverage Jaccard/IoU:\t0.0793\n",
      "\n",
      "[Epoch 003 Training]\tCross-Entropy Loss:\t0.3113\n",
      "[Epoch 003 Validation]\tAverage F1 Score:\t0.1462\tAverage Jaccard/IoU:\t0.0793\n",
      "\n",
      "[Epoch 004 Training]\tCross-Entropy Loss:\t0.2868\n",
      "[Epoch 004 Validation]\tAverage F1 Score:\t0.2747\tAverage Jaccard/IoU:\t0.1617\n",
      "\n",
      "[Epoch 005 Training]\tCross-Entropy Loss:\t0.2076\n",
      "[Epoch 005 Validation]\tAverage F1 Score:\t0.2491\tAverage Jaccard/IoU:\t0.1439\n",
      "\n",
      "[Epoch 006 Training]\tCross-Entropy Loss:\t0.2503\n",
      "[Epoch 006 Validation]\tAverage F1 Score:\t0.1776\tAverage Jaccard/IoU:\t0.0977\n",
      "\n",
      "[Epoch 007 Training]\tCross-Entropy Loss:\t0.2106\n",
      "[Epoch 007 Validation]\tAverage F1 Score:\t0.2516\tAverage Jaccard/IoU:\t0.1445\n",
      "\n",
      "[Epoch 008 Training]\tCross-Entropy Loss:\t0.2679\n",
      "[Epoch 008 Validation]\tAverage F1 Score:\t0.1818\tAverage Jaccard/IoU:\t0.1009\n",
      "\n",
      "[Epoch 009 Training]\tCross-Entropy Loss:\t0.2164\n",
      "[Epoch 009 Validation]\tAverage F1 Score:\t0.2756\tAverage Jaccard/IoU:\t0.1616\n",
      "\n",
      "[Epoch 010 Training]\tCross-Entropy Loss:\t0.1610\n",
      "[Epoch 010 Validation]\tAverage F1 Score:\t0.2442\tAverage Jaccard/IoU:\t0.1399\n",
      "\n",
      "[Epoch 011 Training]\tCross-Entropy Loss:\t0.1747\n",
      "[Epoch 011 Validation]\tAverage F1 Score:\t0.3135\tAverage Jaccard/IoU:\t0.1877\n",
      "\n",
      "[Epoch 012 Training]\tCross-Entropy Loss:\t0.2167\n",
      "[Epoch 012 Validation]\tAverage F1 Score:\t0.2354\tAverage Jaccard/IoU:\t0.1354\n",
      "\n",
      "[Epoch 013 Training]\tCross-Entropy Loss:\t0.1691\n",
      "[Epoch 013 Validation]\tAverage F1 Score:\t0.3439\tAverage Jaccard/IoU:\t0.2139\n",
      "\n",
      "[Epoch 014 Training]\tCross-Entropy Loss:\t0.1374\n",
      "[Epoch 014 Validation]\tAverage F1 Score:\t0.3393\tAverage Jaccard/IoU:\t0.2079\n",
      "\n",
      "[Epoch 015 Training]\tCross-Entropy Loss:\t0.1223\n",
      "[Epoch 015 Validation]\tAverage F1 Score:\t0.3420\tAverage Jaccard/IoU:\t0.2100\n",
      "\n",
      "[Epoch 016 Training]\tCross-Entropy Loss:\t0.1321\n",
      "[Epoch 016 Validation]\tAverage F1 Score:\t0.2942\tAverage Jaccard/IoU:\t0.1740\n",
      "\n",
      "[Epoch 017 Training]\tCross-Entropy Loss:\t0.1684\n",
      "[Epoch 017 Validation]\tAverage F1 Score:\t0.3135\tAverage Jaccard/IoU:\t0.1911\n",
      "\n",
      "[Epoch 018 Training]\tCross-Entropy Loss:\t0.1379\n",
      "[Epoch 018 Validation]\tAverage F1 Score:\t0.2987\tAverage Jaccard/IoU:\t0.1767\n",
      "\n",
      "[Epoch 019 Training]\tCross-Entropy Loss:\t0.1355\n",
      "[Epoch 019 Validation]\tAverage F1 Score:\t0.3418\tAverage Jaccard/IoU:\t0.2086\n",
      "\n",
      "[Epoch 020 Training]\tCross-Entropy Loss:\t0.1283\n",
      "[Epoch 020 Validation]\tAverage F1 Score:\t0.3395\tAverage Jaccard/IoU:\t0.2097\n",
      "\n",
      "[Epoch 021 Training]\tCross-Entropy Loss:\t0.1400\n",
      "[Epoch 021 Validation]\tAverage F1 Score:\t0.3153\tAverage Jaccard/IoU:\t0.1895\n",
      "\n",
      "[Epoch 022 Training]\tCross-Entropy Loss:\t0.1425\n",
      "[Epoch 022 Validation]\tAverage F1 Score:\t0.3201\tAverage Jaccard/IoU:\t0.1936\n",
      "\n",
      "[Epoch 023 Training]\tCross-Entropy Loss:\t0.1364\n",
      "[Epoch 023 Validation]\tAverage F1 Score:\t0.3436\tAverage Jaccard/IoU:\t0.2154\n",
      "\n",
      "[Epoch 024 Training]\tCross-Entropy Loss:\t0.1311\n",
      "[Epoch 024 Validation]\tAverage F1 Score:\t0.3450\tAverage Jaccard/IoU:\t0.2186\n",
      "\n",
      "[Epoch 025 Training]\tCross-Entropy Loss:\t0.1260\n",
      "[Epoch 025 Validation]\tAverage F1 Score:\t0.3079\tAverage Jaccard/IoU:\t0.1841\n",
      "\n",
      "[Epoch 026 Training]\tCross-Entropy Loss:\t0.1207\n",
      "[Epoch 026 Validation]\tAverage F1 Score:\t0.3513\tAverage Jaccard/IoU:\t0.2151\n",
      "\n",
      "[Epoch 027 Training]\tCross-Entropy Loss:\t0.1093\n",
      "[Epoch 027 Validation]\tAverage F1 Score:\t0.3667\tAverage Jaccard/IoU:\t0.2316\n",
      "\n",
      "[Epoch 028 Training]\tCross-Entropy Loss:\t0.0724\n",
      "[Epoch 028 Validation]\tAverage F1 Score:\t0.4309\tAverage Jaccard/IoU:\t0.2886\n",
      "\n",
      "[Epoch 029 Training]\tCross-Entropy Loss:\t0.1402\n",
      "[Epoch 029 Validation]\tAverage F1 Score:\t0.2463\tAverage Jaccard/IoU:\t0.1416\n",
      "\n",
      "[Epoch 030 Training]\tCross-Entropy Loss:\t0.1112\n",
      "[Epoch 030 Validation]\tAverage F1 Score:\t0.3382\tAverage Jaccard/IoU:\t0.2040\n",
      "\n",
      "[Epoch 031 Training]\tCross-Entropy Loss:\t0.0738\n",
      "[Epoch 031 Validation]\tAverage F1 Score:\t0.4705\tAverage Jaccard/IoU:\t0.3108\n",
      "\n",
      "[Epoch 032 Training]\tCross-Entropy Loss:\t0.0549\n",
      "[Epoch 032 Validation]\tAverage F1 Score:\t0.6482\tAverage Jaccard/IoU:\t0.4827\n",
      "\n",
      "[Epoch 033 Training]\tCross-Entropy Loss:\t0.0418\n",
      "[Epoch 033 Validation]\tAverage F1 Score:\t0.6665\tAverage Jaccard/IoU:\t0.5053\n",
      "\n",
      "[Epoch 034 Training]\tCross-Entropy Loss:\t0.0353\n",
      "[Epoch 034 Validation]\tAverage F1 Score:\t0.6537\tAverage Jaccard/IoU:\t0.4924\n",
      "\n",
      "[Epoch 035 Training]\tCross-Entropy Loss:\t0.0333\n",
      "[Epoch 035 Validation]\tAverage F1 Score:\t0.6876\tAverage Jaccard/IoU:\t0.5303\n",
      "\n",
      "[Epoch 036 Training]\tCross-Entropy Loss:\t0.0528\n",
      "[Epoch 036 Validation]\tAverage F1 Score:\t0.5957\tAverage Jaccard/IoU:\t0.4293\n",
      "\n",
      "[Epoch 037 Training]\tCross-Entropy Loss:\t0.0459\n",
      "[Epoch 037 Validation]\tAverage F1 Score:\t0.6018\tAverage Jaccard/IoU:\t0.4365\n",
      "\n",
      "[Epoch 038 Training]\tCross-Entropy Loss:\t0.0376\n",
      "[Epoch 038 Validation]\tAverage F1 Score:\t0.6788\tAverage Jaccard/IoU:\t0.5236\n",
      "\n",
      "[Epoch 039 Training]\tCross-Entropy Loss:\t0.0317\n",
      "[Epoch 039 Validation]\tAverage F1 Score:\t0.6696\tAverage Jaccard/IoU:\t0.5107\n",
      "\n",
      "[Epoch 040 Training]\tCross-Entropy Loss:\t0.0332\n",
      "[Epoch 040 Validation]\tAverage F1 Score:\t0.7488\tAverage Jaccard/IoU:\t0.6030\n",
      "\n",
      "[Epoch 041 Training]\tCross-Entropy Loss:\t0.0300\n",
      "[Epoch 041 Validation]\tAverage F1 Score:\t0.6758\tAverage Jaccard/IoU:\t0.5142\n",
      "\n",
      "[Epoch 042 Training]\tCross-Entropy Loss:\t0.0310\n",
      "[Epoch 042 Validation]\tAverage F1 Score:\t0.6989\tAverage Jaccard/IoU:\t0.5428\n",
      "\n",
      "[Epoch 043 Training]\tCross-Entropy Loss:\t0.0404\n",
      "[Epoch 043 Validation]\tAverage F1 Score:\t0.6787\tAverage Jaccard/IoU:\t0.5219\n",
      "\n",
      "[Epoch 044 Training]\tCross-Entropy Loss:\t0.0269\n",
      "[Epoch 044 Validation]\tAverage F1 Score:\t0.7627\tAverage Jaccard/IoU:\t0.6204\n",
      "\n",
      "[Epoch 045 Training]\tCross-Entropy Loss:\t0.0309\n",
      "[Epoch 045 Validation]\tAverage F1 Score:\t0.7399\tAverage Jaccard/IoU:\t0.5963\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "with experiment.train():\n",
    "    \n",
    "    print(f'Number of training images:\\t{num_train}\\nNumber of validation images:\\t{num_val}')\n",
    "    for epoch in tqdm(range(epochs), desc=f'Training {epochs} epochs'):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        unet.train()\n",
    "\n",
    "        for i, data in enumerate(train_dataloader):\n",
    "\n",
    "            input_images, targets = data\n",
    "\n",
    "            if is_cuda_available:\n",
    "                input_images = input_images.to(device, dtype=input_images_dtype)\n",
    "                targets = targets.to(device, dtype=targets_dtype)\n",
    "\n",
    "            outputs = unet(input_images)\n",
    "\n",
    "            if use_dice_loss:\n",
    "                outputs = outputs[:,1,:,:].unsqueeze(dim=1)\n",
    "                loss = criterion(outputs, targets)\n",
    "            else:\n",
    "                targets = targets.squeeze(dim=1)\n",
    "                loss = criterion(outputs, targets)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        if use_dice_loss:\n",
    "            print(f'[Epoch {epoch+1:03d} Training]\\tDice Loss:\\t\\t{running_loss/(i+1):.4f}')\n",
    "        else:\n",
    "            print(f'[Epoch {epoch+1:03d} Training]\\tCross-Entropy Loss:\\t{running_loss/(i+1):.4f}')\n",
    "        experiment.log_metric(\"Running Loss\", running_loss, epoch=epoch, step=epoch, include_context=False)\n",
    "\n",
    "        unet.eval()\n",
    "        all_f1 = []\n",
    "        all_jaccard = []\n",
    "\n",
    "        for i, data in enumerate(val_dataloader):\n",
    "            accuracy = 0.0\n",
    "            intersect = 0.0\n",
    "            union = 0.0\n",
    "\n",
    "            input_images, targets = data\n",
    "            if is_cuda_available:\n",
    "                input_images = input_images.to(device, dtype=input_images_dtype)\n",
    "                targets = targets.to(device, dtype=targets_dtype)\n",
    "            outputs = unet(input_images)\n",
    "\n",
    "            # round outputs to either 0 or 1\n",
    "#             if not use_dice_loss: outputs = softmax(outputs)\n",
    "            outputs = softmax(outputs)\n",
    "            outputs = outputs[:, 1, :, :].unsqueeze(dim=1).round()\n",
    "\n",
    "            outputs, targets = outputs.data.cpu().numpy(), targets.data.cpu().numpy()\n",
    "\n",
    "    #         pdb.set_trace()\n",
    "            for out, gt in zip(outputs, targets):\n",
    "                f1 = f1_score(targets.reshape(-1), outputs.reshape(-1), zero_division=1)\n",
    "                all_f1.append(f1)\n",
    "                jaccard = jaccard_score(targets.reshape(-1), outputs.reshape(-1))\n",
    "                all_jaccard.append(jaccard)\n",
    "\n",
    "            if i % 100 == 0 and epoch in [25, 50, 75, 100, 125]:\n",
    "                for idx, (out, gt) in enumerate(zip(outputs, targets)):\n",
    "                    with warnings.catch_warnings():\n",
    "                        img = create_canvas(out, gt, show=False)\n",
    "                        warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "                        experiment.log_image(img, name=f'epoch_{epoch:03d}_batch_{i:03d}_idx_{idx}_segmap', overwrite=True, \n",
    "                                             image_format=\"png\", image_scale=1.0, image_shape=None, image_colormap=\"gray\",\n",
    "                                             image_channels=\"first\", copy_to_tmp=False, step=epoch)\n",
    "        \n",
    "        if params['scheduler'] == 'ReduceLROnPlateau': \n",
    "            scheduler.step(np.mean(all_f1))\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        print(f'[Epoch {epoch+1:03d} Validation]\\tAverage F1 Score:\\t{np.mean(all_f1):.4f}\\tAverage Jaccard/IoU:\\t{np.mean(all_jaccard):.4f}\\n')\n",
    "        \n",
    "        experiment.log_metric('Validation Average F1 Score', np.mean(all_f1), \n",
    "                              epoch=epoch, include_context=False)\n",
    "        experiment.log_metric('Validation Average Jaccard/IoU', np.mean(all_jaccard), \n",
    "                              epoch=epoch, include_context=False)\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
